\chapter{Conclusiones}\label{chapter6}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter5/Figs/}}


En este trabajo se propuso la integración de un modelo causal en un
algoritmo de aprendizaje por refuerzo.
La investigación presenta
elementos para responder a las preguntas de investigación planteadas en el capítulo \ref{chapter1}. Entre las principales contribuciones están: 1) la representación de las relaciones causales presentes en el ambiente, en un grafo causal entre acciones y estados del mundo, y 2) incorporar conocimiento causal en aprendizaje por refuerzo, en particular al
algoritmo Q-learning.
% un esquema de interacción entre la estructura causal y el algoritmo Q-learning. 
% De acuerdo con los experimentos se verifica empíricamente
% la mejora del desempeño de un agente usando diferentes niveles de información
% en la estructura causal (completa y parcialmente incorrecta).
Además, en el capítulo \ref{chapter5} se presenta la experimentación que verifica empíricamente
    la mejora del desempeño de un agente usando diferentes niveles de información (completa y parcialmente incorrecta) contenida en el modelo causal. 

\section{Conclusiones}
    De acuerdo con los resultados de los experimentos 
    se ha llegado a las siguientes conclusiones:
    
\begin{enumerate}
    \item Tener el modelo causal completo parece resolver la tarea. Sin embargo, en la práctica se espera tener un modelo causal incompleto e incorrecto, en donde el aprendizaje por refuerzo es necesario y además el modelo de todos modos asiste a éste.
    \item Proveer a un agente con un grafo que conserva pocas relaciones
    causales verdaderas no afecta tan drásticamente el desempeño del agente, pues 
    continúa teniendo un comportamiento mejor que sin guiar su selección de acciones.
    \item Reducir la probabilidad de consultar el grafo causal en un etapa 
    temprana de entrenamiento, es decir, dar más peso a la explotación que a la exploración, parece no afectar el desempeño del agente demasiado. Esto podría
    ser porque con la poca exploración guiada que han ejecutado los algoritmos,
    ya han sesgado su comportamiento.
    \item El comportamiento utilizando observaciones de alta dimensión para entrenar a un agente y ayudándolo con un modelo en una dimensión mucho menor y con variables que representan al estado del ambiente en un alto nivel mantiene las mismas características que trabajar en un espacio de menor dimensión directamente.
    \item Se dotó a un agente de RL con conocimiento causal para tomar decisiones y planear anticipadamente. Y aunque queda fuera del alcance de esta investigación explotar todas las ventajas de un modelo causal sobre uno asociativo, por ejemplo explicar o imaginar, se motiva a seguir el camino de equipar agentes con herramientas de razonamiento causal en vez de los enfoques que no capturan relaciones causales.
\end{enumerate}

\section{Trabajo futuro}

Diferentes tareas y problemas permanecen abiertos y fuera del alcance de este trabajo. Algunos de ellos con respecto al nivel de formalización de las ideas presentadas, a los escenarios experimentales atacados  y otros que se han encontrado durante la investigación. A continuación, se 
mencionan algunos tareas y problemas pendientes.

Con respecto a los problemas atacados en el trabajo,
varios experimentos y análisis quedan pendientes. Entre ellos, 
para el problema de los interruptores falta 
definir una recompensa óptima por episodio y 
calcular quién tiene la racha más larga como en el dominio del taxi.  Otro escenario que sería interesante
explorar es aquel donde hay estructuras con caminos causales con más de dos aristas. Por ejemplo, para el problema de los interruptores, se pueden
tener escenarios donde sea necesario mover un interruptor ``maestro'' para
poder mover los otros interruptores. Además, queda abierto el problema
de las estructuras uno-a-uno y el comportamiento anormal, en comparación
a las otras configuraciones, de sus curvas de recompensa.

Un problema abierto, es adaptar, en caso de que se pueda, el método propuesto para 
tareas de control continuo. En tareas de esta naturaleza no es trivial definir que una acción 
causa o no cierto efecto en un estado. Principalmente porque las acciones se encuentran
en el dominio de los reales y no se simplifica a decir que la acción se lleva a cabo o no.
Además, otro aspecto a explorar es el reutilizar conocimiento causal. Por ejemplo, 
para problemas del mismo dominio, pero cada problema puede tener distinta meta, sistema
de recompensa o incluso cambiar elementos del modelo de transición.

Por otro lado, sigue abierta la tarea del proceso de aprendizaje del modelo
causal. De manera general, una propuesta consiste en iniciar con una estructura con pesos que representen 
una distribución de probabilidad de creencias de que realizar una acción $a$ cause un estado $x$, i.e., $P_{a\rightarrow x}$. Estos pesos (probabilidades) se pueden
ir actualizando conforme el agente interactúe con el mundo y
a través de sus respuestas le brinde información del modelo causal que lo gobierna.

Por otra parte, también se está intentando atacar tareas más 
complejas que subsumen los siguientes problemas: 1) debido al problema de la asignación de crédito es difícil saber qué acción a la larga causará un estado deseado, 2) los efectos de una acción en el ambiente puede ser imperceptibles, por lo que es necesario medir el efecto causal de una acción sobre un estado una función similar a una función de valor. En particular, un ejemplo de un ambiente con estas dificultades, es Animal AI \cite{beyret2019animalai}.




