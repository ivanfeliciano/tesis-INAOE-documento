\chapter{Conclusiones}\label{chapter6}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter5/Figs/}}

En este trabajo se propuso la integración de un modelo causal en un
algoritmo de aprendizaje por refuerzo.
La investigación presenta
elementos para responder a las preguntas de investigación planteadas en el capítulo \ref{chapter1}. Entre las principales contribuciones están: 1) la representación de las relaciones causales presentes en el ambiente, en un grafo causal entre acciones y estados del mundo, y 2) un esquema de interacción entre la estructura causal y el algoritmo Q-learning. 
% De acuerdo con los experimentos se verifica empíricamente
% la mejora del desempeño de un agente usando diferentes niveles de información
% en la estructura causal (completa y parcialmente incorrecta).
Además, en el capítulo \ref{chapter5} se presenta la experimentación que verifica empíricamente
    la mejora del desempeño de un agente usando diferentes niveles de información (completa y parcialmente incorrecta) contenida en el modelo causal. De acuerdo con los resultados de los experimentos 
    se ha llegado a las siguientes conclusiones:
    
\begin{enumerate}
    \item Tener el modelo causal completo parece resolver la tarea. Surge la 
    duda de si el aprendizaje por refuerzo todavía es necesario.
    \item Proveer a un agente con un grafo que conserva pocas relaciones
    causales verdaderas no afecta tan drásticamente el desempeño del agente, pues 
    continúa teniendo un comportamiento mejor que sin guiar su selección de acciones.
    \item Reducir la probabilidad de consultar el grafo causal en un etapa 
    temprana de entrenamiento, es decir, dar más peso a la explotación que a la exploración, parece no afectar el desempeño del agente demasiado. Esto podría
    ser porque con la poca exploración guiada que han ejecutado los algoritmos,
    ya han sesgado su comportamiento.
\end{enumerate}

\section{Trabajo futuro}

Diferentes tareas y problemas permanecen abiertos y fuera del alcance de este trabajo. Algunos de ellos con respecto a los escenarios experimentales atacados  y otros que se han encontrado durante la investigación. A continuación, se 
mencionan algunos tareas y problemas pendientes.

Con respecto a los problemas atacados en el trabajo,
varios experimentos y análisis quedan pendientes. Entre ellos, 
establecer cuándo se considera resuelta una tarea, por ejemplo, 
definir una recompensa óptima por episodio y 
calcular quién tiene la racha más larga. Con esta información 
se podrían realizar pruebas estadísticas. Otro escenario que sería interesante
explorar es aquel donde en la estructuras con caminos causales con más de dos aristas. Por ejemplo, para el problema de los interruptores, se pueden
tener escenarios donde sea necesario mover un interruptor ``maestro'' para
poder mover los otros interruptores. Además, queda abierto el problema
de las estructuras uno-a-uno y el comportamiento anormal, en comparación
a las otras configuraciones, de sus curvas de recompensa.

Por otro lado, sigue abierta la tarea del proceso de aprendizaje del modelo
causal. De manera general, una propuesta consiste en iniciar con una estructura con pesos que representen 
una distribución de probabilidad de creencias de que realizar una acción $a$ cause un estado $x$, i.e., $P_{a\rightarrow x}$. Estos pesos (probabilidades) se pueden
ir actualizando conforme el agente interactúe con el mundo y
a través de sus respuestas le brinde información del modelo causal que lo gobierna.


Por otra parte, también se está intentando atacar tareas más 
complejas que subsumen los siguientes problemas: 1) debido al problema de la asignación de crédito es difícil saber qué acción a la larga causará un estado deseado, 2) los efectos de una acción en el ambiente puede ser imperceptibles, por lo que es necesario medir el efecto causal de una acción sobre un estado una función similar a una función de valor. En particular, un ejemplo de un ambiente con estas dificultades, es Animal AI \cite{beyret2019animalai}.


