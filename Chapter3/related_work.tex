\chapter{Trabajo relacionado}\label{chapter3}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter3/Figs/}}


% El aprendizaje por refuerzo y la inferencia
% causal han evolucionado de manera 
% independiente y prácticamente sin 
% interacción alguna. Sin embargo, ambos campos
% pueden estar relacionados en algunos
% procesos de solución de problemas.
% Por lo tanto, 
Trabajos recientes
se han enfocado en conectar ideas del aprendizaje por refuerzo y la inferencia causal
\cite{Gershman2017, 6-DBLP:journals/midm/YuDLR19, lu2018deconfounding, dasgupta2019causal}. Ambos campos
pueden estar relacionados o complementarse entre sí en algunos procesos de solución de problemas. 
% \hl{
% Estos trabajos buscan desde, entender, a través
% del paradigma de RL, como los humanos y algunos animales aprenden 
% un modelo causal de su ambiente para utilizarlos en la planificación 
% de las secuencias de sus acciones; hasta intentar resolver el problema 
% de que la mayoría de los  
% procesos de aprendizaje en RL son como ``cajas negras'' que ignoran
% conocimiento importante codificado en relaciones causales entre eventos.
% }
El enfoque que combina ambas áreas, es conocido como
\textit{aprendizaje por refuerzo causal} \cite{CausalRL2019EliasB, chaochao_2019}. Éste busca resolver tareas una vez establecida la conexión entre RL y CI.
% De acuerdo con \citet{CausalRL2019EliasB}, 
Algunas problemas abiertos y oportunidades de aprendizaje en esta área son: 

\begin{itemize}
    \item \textit{Explicabilidad} en las acciones de los humanos e \textit{interpretabilidad} en los sistemas de RL en términos causales \cite{Gershman2017, madumal2019explainable, edmonds2018human}
    \item Aprender una política, a través de la combinación sistemática de modos de interacción \textit{en línea} y \textit{fuera de línea} \cite{CausalRL2019EliasB}.
    \item Refinar el espacio de acciones, a través de conocer cuándo y dónde intervenir; por ejemplo, a través de la identificación de acciones con una recompensa mayor \cite{lattimore2016causal} o 
    usando los parámetros de un modelo causal \cite{playingagainstnature2018}.
    \item Manipular la función de optimización o de recompensa basándose en la intención \cite{CausalRL2019EliasB, everitt2019reward}. 
    \item  Generalización de la política. Se busca una política general
    basándose en que no hay cambios en una estructura causal \cite{CausalRL2019EliasB}.
    \item Aprendizaje de modelos causales. Se busca descubrir la estructura causal
    o los parámetros de un modelo ya sea con observaciones \cite{lu2018deconfounding, zhu2019causal}
    o experimentos \cite{nair2019causal, gonzalezsoto2020causal, dasgupta2019causal}.
    % \item  Aprendizaje causal por imitación: Se aprende una política a través 
    % de interacciones y a través de datos dados por un experto con
    % una función de recompensa desconocida \cite{nair2019causal}.
\end{itemize}

    
% La meta de estos trabajos es mostrar que el RL puede ser más robusto y general a través de mecanismos
% causales y viceversa.

%  yet it still has major weaknesses when compared toanimate intelligence. Two major issues can be stated in terms of questions
 
%  Question 1: why is RL on the original high-dimensional ATARI games harder than on downsampled versions
%   Animals likely have methods to identify objects (in computer game lingo, “sprites”) bygrouping pixels according to “common fate” (known from Gestalt psychology) or common response to intervention.This question thus is related to the question of what constitutes an object, which concerns not only perception butalso concerns how we interact with the world. We can pick up one object, but not half an object. Objects thus alsocorrespond to modular structures that can be separately intervened upon or manipulated. 
 
%  Question 2: why is RL easier if we permute the replayed data?A
%  temporal order contains information that animateintelligence uses.  Information is not only contained in temporal order, but also in the fact that slow changes of thestatistics effectively create a multi-domain setting. Multi-domain data have been shown to help identify causal (and thusrobust) features, and more generally in the search for causal structure, by looking for invariances 
 
% Hasta donde sabe el autor de esta investigación, este nuevo enfoque propuesto no ha sido sido formulado de tal manera que se sustente formalmente.
% Sin embargo, los esfuerzos que se han hecho desde esta perspectiva, han abierto
% nuevos problemas y parece ser el camino correcto para solucionar algunos otros
% que están siendo estudiados.
% ni el problema de aprendizaje por 
% refuerzo ni el del enfoque de causalidad en este trabajo, han 


En general, algunas tareas mencionadas previamente se pueden dividir en dos áreas. La primera busca usar razonamiento causal en aprendizaje por refuerzo y la segunda tiene como objetivo descubrir relaciones causales utilizando aprendizaje por refuerzo. En las siguientes dos secciones se describen
algunos trabajos de los enfoques en ambas direcciones. Algunas de las
propuestas descritas no están del todo alineadas con esta investigación, sin embargo, es un área que recientemente está siendo explorada y los resultados de éstas parecen prometedores.
Finalmente, en la última sección se mencionan algunas propuestas que, de manera similar a este trabajo, buscan guiar la selección de acciones en un agente de RL. No obstante, la principal diferencia con este trabajo es que los métodos expuestos en esa última sección no utilizan información causal.

% This chapter should include a comparison of your work
% with closely related efforts. It should demonstrate the principal differences and
% similarities with respect to (1) the details of the problem, (2) the approach, (3) the
% results, and possibly (4) the methodology. In particular, when you compare your
% approach with those of others, it is important that you objectively weigh the advantages
% and disadvantages. This increases your credibility and that of your study. Further,
% it will show that you did your homework, and that you know what the state-of-the-art
% in the area is. This will support and strengthen any claims of originality in your work.

% DEbo hacer una lista de los artículos que más me interesan


\section{Usando razonamiento causal en aprendizaje por refuerzo}


\citet{Gershman2017} establece que el modelo usado en los algoritmos de aprendizaje por refuerzo basados en modelo, es causal.
Ellos describen algunas relaciones causales fundamentales en el aprendizaje por refuerzo. Tomar una acción en un estado causa una recompensa y una transición a un nuevo estado.
En general, estas relaciones se resumen en la tabla \ref{table:causal-relationships}.

\begin{table}[h]
\centering
\caption{Relaciones causales en RL según \cite{Gershman2017}.}
\label{table:causal-relationships}
\begin{tabular}{@{}ll@{}}
\toprule
Causas         & Efecto     \\ \midrule
estado, acción & estado     \\
estado, acción & recompensa \\ \bottomrule
\end{tabular}
\end{table}

% A pesar de que esta perspectiva tiene sentido y es natural para un ser humano, no se aborda o explica el mecanismo intervencionista de la inferencia causal. 
% Por otra parte, en esta tesis, más que plantear un algoritmo de RL 
% basado en modelo en términos de causales, se
% busca complementar el aprendizaje por refuerzo con información causal.
% Por otra parte, nuestro propuesta, a diferencia de \cite{Gershman2017}, está basada en relaciones entre variables de acción y otras que no necesariamente son la informacióń directa que envía el ambiente a un agente. Aunque GErshman habla de los modelos ocultos.

% Los autores en \cite{lu2018deconfounding} presentan una método formulado para resolver problemas de RL con datos observacionales.

% Los autores
% presentan un grafo causal donde reemplazan a los estados 
% por variables latentes de los estados en un espacio de menor dimensión. Además, añaden factores
% de confusión no observables (aquellas variables que afectan a la acción y la salida). Sin embargo, ya que el objetivo de ese trabajo es manejar los factores de confusión en problemas con datos observacionales algunos elementos causales 
% quedan implícitos. Por ejemplo, los parámetros del modelo causal quedan codificados en aproximadores de funciones y no es claro si los patrones aprendidos tienen una perspectiva más asociativa que causal. En este trabajo de tesis, 
% se ataca un problema más pequeño y sin intentar construir un puente entre el 
% problema completo de RL y la causalidad. Pero se toma la idea de trabajar con
% relaciones causales entre variables de acción y variables que representen 
% estados en un espacio menor.

\citet{lu2018deconfounding} presentan un esquema de relaciones causales similares al de la tabla \ref{table:causal-relationships} a lo largo varios pasos en el tiempo de aprendizaje. Ellos atacan el problema de aprender buenas políticas a partir de datos observacionales en la cual factores no observados afectan las acciones observadas y recompensas \cite{pearl_2009}. El método propuesto consiste de manera general en dos pasos: aprender un modelo generador \cite{jebara2012machine} sobre un espacio de variables latentes, es decir, las observaciones son mapeadas a un espacio de menor dimensión.  Este modelo de variables latentes aprendido a partir de datos observacionales permite descubrir los factores ocultos e inferir como afectar las acciones y recompensas. Entonces, se aprende una política basándose en este modelo. 
% Se puede explotar el modelo aprendido para generar trayectorias para evaluación de políticas y optimización.  
% Se estima la función de transición con el modelo generador y se calcular la recompensa. Con  se generan trayectoria con el la función estimada para entrenar la política. 
El método de RL que utiliza el modelo que aprende los factores ocultos se desempeña mejor que aquel sin el cálculo de estos. La principal conexión que se hace entre aprendizaje por refuerzo y causalidad es calcular las recompensas en el modelo generador aprendido usando el cálculo $do$. A pesar de que este trabajo propone aprovechar información causal para calcular las recompensas, está enfocado en tareas con datos observacionales. 
Por otra parte, en esta tesis, la política no es aprendida a partir de observaciones, sino de actuar directamente sobre el ambiente.

% Por otra parte, en esta tesis se propone auxiliar el aprendizaje con conocimiento previo sobre una tarea para un agente que interactúa con un ambiente directamente.
% % consider the problem of learning goodpolicies solely from historical data in which unobserved factors (confounders) affect bothobserved actions and rewards.
% También se han atacado problemas clásicos de RL 
% en configuraciones observacionales donde existen factores de confusión. Los autores en \cite{bareinboim2015bandits, lu2018deconfounding}
% han abordado el manejo de estos factores de confusión .

La idea de utilizar conocimiento de modelos
causales para evitar o reducir el aprendizaje a 
prueba y error en RL es un área con escasa 
exploración pero prometedora.
\citet{lattimore2016causal},
explotan información causal en el problema
del bandido y muestran de qué manera, a través
de intervenciones, se puede mejorar la
tasa en la que se identifican las acciones
con una recompensa más alta. A pesar de atacar un problema más pequeño
que el que se plantea en un MDP, su propuesta motiva a utilizar 
conocimiento causal para tomar decisiones.

Desde un enfoque hacia la teoría de decisiones, \citet{playingagainstnature2018} proponen una alternativa al aprendizaje por interacción que se sigue en RL en ambientes causales. Los autores plantean que la mayoría de los métodos en RL son meramente asociativos y que se puede aprovechar la característica de algunos ambientes gobernados por mecanismos causales. El método que proponen consiste en seleccionar la mejor acción 
	en cada interacción usando los parámetros de un modelo causal. Estos últimos se van aprendiendo utilizando las respuestas del ambiente. De acuerdo con los resultados, su propuesta tiene un desempeño similar a un método clásico de RL, sin embargo, el primero explota la información causal del mundo. Por otra parte, una suposición que comparte con esta tesis, es conocer la estructura causal de variables del ambiente. La principal diferencia es que ellos siguen un enfoque de
	\textit{teoría de decisiones} \cite{sep-decision-theory}. En éste un agente toma de decisiones bajo incertidumbre no necesariamente utilizando la perspectiva del refuerzo y esta tesis se concentra en guiar la selección de acciones de un agente de RL sin involucrar descubrimiento causal.

Otro problema en RL que está siendo atacado con elementos causales es el de manipulación de la función de recompensa (\textit{reward tampering} en inglés) \cite{everitt2019reward}. Descrito de manera general, este problema surge cuando un agente 
% descubre una estrategia que parece la mejor, pero en realidad encontró una ``laguna'' en la recompensa especificada. A veces, un agente 
se enfoca en recoger pequeñas recompensas y evitar aquel comportamiento que lo lleva por la recompensa mayor. Por ejemplo, un agente que debe recoger diamantes, pero existen rocas que también le dan recompensa puede inclinarse a solo buscar las rocas y no los diamantes. Los autores utilizan un diagrama de influencia causal para representar este problema. Los nodos de utilidad representan las recompensas y los nodos de decisión las acciones. Por otro lado, el resto de nodos son los estados que describen las recompensas obtenidas (por ejemplo, si lo recogido es una roca o un diamante).
La propuesta de esta tesis, no modela como afectan las acciones a las recompensas ni como puede la información con respecto a las recompensas influir en la selección de una acción, sino que se enfoca en usar informacióń de las relaciones entre acciones y estados.

En el trabajo de \citet{nair2019causal}, se propone
aprender un grafo causal usando aprendizaje supervisado y
entrenar una política con ayuda del grafo y mediante aprendizaje por imitación, para generalizar sobre 
configuraciones diferentes de una misma
tarea.
El ambiente utilizado en ese trabajo es básicamente el mismo que se utiliza en esta
tesis, salvo por algunas modificaciones. Entre ellas, simular un ambiente estocástico al no llegar siempre al estado deseado y cambios en el esquema de reinicio en los estados meta en cada episodio. Sin embargo, los problemas a atacar y los enfoques
de solución son diferentes. Ellos aprenden el grafo con aprendizaje supervisado y
la política con aprendizaje por imitación. En esta tesis, se supone que
el modelo causal es dado y que es información extra que sirve como apoyo a un agente de RL.


Por otra parte, existe la pregunta de si es posible para los modelos modernos de aprendizaje por refuerzo adquirir conocimiento causal similar al humano. Para responder a esta interrogante,  los autores en \cite{edmonds2018human} comparan el desempeño de humanos y de una algoritmo de RL en una tarea diseñada por los autores para examinar el aprendizaje de secuencias de acciones regidas por estructuras causales.  De acuerdo con sus resultados, el algoritmo de RL es incapaz de capturar los mecanismos causales. Esto motiva a encontrar la diferencia entre el aprendizaje causal humano y el RL. De los resultados que se obtuvieron en esta tesis se puede intuir lo que se ve en ese trabajo. A veces, tener el modelo causal que gobierna al ambiente es suficiente sin necesidad de un aprendizaje basado en la optimización de una recompensa. Sin embargo, a veces no es posible 
tener toda la información del mundo del agente, por lo tanto, 
se puede apoyar el aprendizaje de una política con poca información
extra accesible para el agente.

En general, 
los trabajos mencionados en esta sección, se enfocan en 
utilizar herramientas de la inferencia causal sobre
métodos de RL.
Se atacan desde el problema
de seleccionar una acción buscando optimizar la recompensa
o utilizando los parámetros de un modelo causal, hasta 
intentar aprender una política general para tareas que tienen
una estructura causal similar.
De manera similar a algunos de estos trabajos, en esta
investigación se busca reducir el espacio de exploración durante el paso de selección de acción. La propuesta de este trabajo
es una alternativa a los demás enfoques que también buscan
una selección de acciones guiada de alguna forma, limitando así la posibilidad de cometer errores de manera recurrente.

\section{Descubrimiento causal usando aprendizaje por refuerzo}

El descubrir la estructura causal entre un conjunto de variables es un problema
fundamental en diferentes ciencias. A pesar de que 
el alcance de esta tesis no cubre la parte de descubrimiento causal, a continuación se mencionan brevemente algunos trabajos 
que han logrado mostrar que el razonamiento causal puede surgir a partir del aprendizaje por refuerzo \cite{dasgupta2019causal, madumal2019explainable, zhu2019causal}. 


Existe el problema de descubrir la estructura causal de un conjunto de variables. El trabajo  de \citet{zhu2019causal} propone utilizar aprendizaje por refuerzo para buscar un grafo acíclico dirigido con el puntaje más alto. En vez de que la meta sea aprender una política, se usa el RL como estrategia de búsqueda donde la salida final es una grafo. Para la generación de este grafo, se consideran las condiciones de Markov, minimalidad y fidelidad \cite{spirtes2000causation}. La búsqueda es sobre todos los grafos generados durante el entrenamiento con la mejor recompensa. La recompensa está diseñada para incorporar una función de puntaje y dos términos de penalidad para forzar que no existan ciclos en el grafo.
% Mencionar las suposiciones en el proceso de 
% generación delos datos
% para que la estructura sea identificable.
% We also assume causal minimality, which in thiscase reduces to that each functionfiis not a constant in any of its arguments (Peters et al., 2014).Without further assumption on the forms of functions and/or noises, the above model can be identifiedonly up to Markov equivalence class under the usual Markov and faithful assumptions

\citet{dasgupta2019causal} proponen descubrir una estructura causal a partir de datos.
% La idea general es mostrar que el razonamiento causal puede surgir de RL.
% Se propone entrenar un agente que haga razonamiento causal a través del uso de meta
% aprendizaje por refuerzo. 
Se entrena una red recurrente (RNN) con RL para resolver
una serie de problemas que contienen estructuras causales. Además de la codificación del modelo causal, se encontró que el agente
puede realizar razonamiento causal en situaciones no vistas de manera que se obtengan
recompensas positivas. El agente puede seleccionar intervenciones informativas, hacer inferencia
causal a partir de datos observacionales y hacer predicciones contrafactuales. 
% Con respecto a la tesis, este método difiere principalmente porque se enfoca en mostrar que el razonamiento causal puede surgir de aprendizaje por refuerzo sobre datos sintéticos y no en la dirección de usar datos para apoyar el aprendizaje de RL.

% En \cite{bengio2019metatransfer} se propone un aprender estructuras 
% causales basándose en qué tan rápido se adapta un agente a nuevas
% distribuciones. Esto, bajo la suposición de que cuando el conocimiento de una distribución está representado apropiadamente, entonces los cambios son menores.

Otro trabajo enfocado a aprovechar aprender y explotar un modelo causal para brindar a un agente la capacidad de hacer razonamiento causal y completar tareas dirigidas por metas en ambientes donde las observaciones son imágenes se presenta en \cite{nair2019causal}. Los autores proponen atacar el problema en dos fases, una de inducción causal, donde el agente descubre las relaciones de causa y efecto a través de la realización de acciones y la observación de las salidas y la segunda fase de inferencia causal donde el agente utiliza las relaciones causales adquiridas para guiar sus acciones para completar una tarea. Se propone atacarlo como un problema de meta aprendizaje. Primero, se aprende de manera supervisada un modelo de inducción causal que recibe como entrada una trayectoria de datos observacionales y la salida es un grafo que captura la estructura causal. Después, la estructura causal es utilizada para aprender la política condicionada a la meta, la cual aprende a utilizar el modelo para completar la meta específica.	
% Aunque el aprendizaje de modelo que produce la estructura causal tiene un enfoque asociativo, 
La naturaleza de la tarea condicionada a metas sobre ambientes gobernados por un modelo causal inspiró parte de los experimentos de esta tesis. En específico, se utiliza el mismo ambiente (interruptores de luz) para evaluar el método que se propone en este documento. Por otra parte, esta tesis se limita a la fase de aprovechar la estructura causal para guiar a una agente y aprender una función de valor de acción. Además, la principal diferencia es la forma en que 
se consulta el modelo causal, mientras que en el \citet{nair2019causal} utilizan 
modelos de atención para codificar el grafo, en este trabajo, se cuenta con la 
estructura explícita.
% Por otro lado, este trabajo se concentra en utilizar modelos de atención para aprender una política dado el grafo y usando aprendizaje por imitación. \hl{La prin}


Además de los enfoques de descubrimiento causal mencionados también existe la tarea de hacer sistemas inteligentes más transparentes, interpretables y explicables. \citet{shi2020selfsupervised}-y
\citet{madumal2019explainable} se concentran en la explicación, una justificación las decisiones y acciones que toma el sistema. Toman como base la idea de que las personas ven al mundo a través de lentes causales, esto es, construyen relaciones causales para actuar en el mundo y entender nuevos eventos y explicar eventos. Por lo que \citet{madumal2019explainable} proponen codificar un modelo causal entre variables de interés. introducen el modelo de influencia de acción para agentes de RL. Este modelo de influencia de acción se aproxima al modelo causal del ambiente relativo a las acciones que toma el agente. De manera muy general, su modelo consiste de un conjunto de variables que describen el estado del ambiente donde cada variable tiene un conjunto de ecuaciones estructurales que describen el efecto de una acción y otras variables de estado sobre ellas. Los autores proponen que se provea un DAG donde los nodos representan variables de estado y las aristas las acciones. Las ecuaciones que corresponden a cada acción sobre cada variable se aprende con modelos de regresión usando datos que se van obteniendo a partir de experiencias. Los autores evalúan los resultados usando humanos que califican qué tan bien los modelos aprendidos explican las acciones de los agentes. La principal diferencia con el trabajo de esta tesis es que ellos se concentran en aprender las ecuaciones estructurales para hacer consultas posteriores al aprendizaje. Por otra parte, se comparte la suposición de contar con el modelo causal de antemano.

Como se dijo al principio de esta sección, la 
parte del descubrimiento causal queda fuera
de esta investigación. Sin embargo, algunos de los trabajos descritos, proveen algunas ideas que 
pueden ayudar a extender esta tesis. Por ejemplo,
cubrir aspectos en la parte de las suposiciones
que se deben hacer para que el aprendizaje de un modelo sea realmente desde un enfoque causal o proponer alternativas
a la codificación de la estructura causal.


% ausal reasoning has been an indispensable capability for humans and other in-telligent animals to interact with the physical world. In this work, we propose toendow an artificial agent with the capability of causal reasoning for completinggoal-directed tasks.  We develop learning-based approaches to inducing causalknowledge in the form of directed acyclic graphs, which can be used to contex-tualize a learned goal-conditional policy to perform tasks in novel environmentswith latent causal structures.  We leverage attention mechanisms in our causalinduction model and goal-conditional policy, enabling us to incrementally generatethe causal graph from the agent’s visual observations and to selectively use theinduced graph for determining actions.  Our experiments show that our methodeffectively generalizes towards completing new tasks in novel environments withpreviously unseen causal structures


\section{Exploración guiada en aprendizaje por refuerzo}

Un problema fundamental en los algoritmos de aprendizaje por refuerzo
es el balance entre la exploración del ambiente y la explotación
de la información con la que cuenta el agente y existen diversas técnicas para lidiar con este compromiso.
Las estrategias de explotación y exploración no 
son dirigidas y no buscan explícitamente transiciones
interesantes \cite{mcfarlane2018survey}.
Sin embargo, según \citet{hafner2019dream}, usar modelos de predicción parece una manera 
prometedora para lidiar con este problema.
 
\citet{mazumder2019guided} han propuesto una técnica reciente para una exploración 
guiada, sin apoyo de un modelo causal.
Los autores proponen un nuevo método
para acelerar el entrenamiento de algoritmos de RL con el uso de una propiedad presente
en algunos problemas, la cual denominan \textit{permisibilidad estado-acción}.
La idea principal es tener un clasificador
que guíe el paso de la selección de acciones. Éste clasifica si se llega
a una solución óptima dada la acción y el
estado actual. Sin embargo, el enfoque sigue siendo
asociativo.

\citet{saunders2017trial} proponen un esquema 
para una exploración segura, i.e., se busca evitar estados y acciones catastróficas \cite{Geibel01reinforcementlearning}, a través de la intervención de un humano. El humano controla la interfaz entre el agente y el ambiente, vigila
al agente y bloquea cualquier acción que pueda ser catastrófica. Este enfoque
está limitado a la necesidad de un humano para intervenir durante la interacción 
del agente con su ambiente.


\citet{Abel2015GoalBasedAP} se enfocan en aprovechar conocimiento
brindado por un humano experto acerca de las acciones. Los autores presentan
una formalización de este conocimiento que permite al agente 
evitar acciones subóptimas y acotar la búsqueda al eliminar acciones 
irrelevantes para la meta del agente. A pesar de buscar reducir el espacio 
de búsqueda, la principal diferencia con esta tesis es que
los autores en \cite{Abel2015GoalBasedAP} suponen que se conoce el modelo 
del problema, es decir, se cuenta con la función de transición y por lo tanto,
pueden resolver la tarea usando programación dinámica.


Un enfoque diferente para superar el problema de la exploración aleatoria es
aprovechar demostraciones realizadas por humanos. \citet{nair2017overcoming} muestran que se puede acelerar el aprendizaje por refuerzo al incorporar demostraciones en tareas donde el control es continuo. Si bien, en esta tesis se atacan problemas donde la información extra es provista por un experto, la principal diferencia es que la información provista por el experto en esta tesis se limita a definir las conexiones en la estructura, mientras que en el trabajo de \citet{nair2017overcoming}, requiere una enorme cantidad de interacciones que sirven para entrenar el modelo propuesto. Además, esta tesis se limita a espacios de acciones discretas.


Por otro lado, en el aprendizaje por refuerzo profundo existe el problema de que las experiencias del búfer de repetición son muestreadas de manera uniforme sin tomar en cuenta su importancia. \citet{schaul2015prioritized}  se enfocan en priorizar aquellas transiciones por lo que hacen que las experiencias sean más eficientes y efectivas que muestrear de manera aleatoria. El usar un búfer de experiencias conduce a dos opciones de diseño en dos niveles: el primero con respecto a qué experiencias almacenar y el segundo a cuáles experiencias utilizar. Los autores en \cite{schaul2015prioritized} se enfocan en la segunda opción mientras que la tesis en la primera. En esta tesis se hace la suposición que guardar mejores experiencias también conduce a un aprendizaje donde se alcanza una mayor recompensa en un menos pasos de entrenamiento.

Como se puede ver en los trabajos mencionados,
existen muchas formas de guiar
a un agente de RL, donde no necesariamente
se cuenta con información causal. Sin embargo, 
desde la perspectiva muy subjetiva del autor de este trabajo, 
se debe buscar 
brindar la capacidad de aprender y/o
utilizar información causal a los agentes que atacan problemas de toma de decisiones. Evaluar
cambios en la consecuencias dados cambios en 
las causas, conocer el por qué se eligió cierta
secuencia de decisiones, son aspectos
muy importantes en problemas del \textit{mundo real}.

% Hablar de los world model
% he program to move statistical learning towards causal learning has links to reinforcementlearning (RL), a sub-field of machine learning. RL used to be (and still often is) considered a field that has trouble withreal-world high-dimensional data, one reason being that feedback in the form of a reinforcement signal is relativelysparse when compared to label information in supervised learning. The DeepQ agent (Mnih et al., 2015) yielded resultsthat the community would not have considered possible at the time, yet it still has major weaknesses when compared toanimate intelligence. Two major issues can be stated in terms of questions (Schölkopf, 2015, 2017):Question 1: why is RL on the original high-dimensional ATARI games harder than on downsampled versions?Forhumans, reducing the resolution of a game screen would make the problem harder, yet this is exactly what was done tomake the DeepQ system work. Animals likely have methods to identify objects (in computer game lingo, “sprites”) bygrouping pixels according to “common fate” (known from Gestalt psychology) or common response to intervention.This question thus is related to the question of what constitutes an object, which concerns not only perception butalso concerns how we interact with the world. We can pick up one object, but not half an object. Objects thus alsocorrespond to modular structures that can be separately intervened upon or manipulated.  The idea that objects aredefined by their behavior under transformation is a profound one not only in psychology, but also in mathematics, cf.Klein (1872); MacLane (1971)


% Question 2: why is RL easier if we permute the replayed data?As an agent moves about in the world, it influences thekind of data it gets to see, and thus the statistics change over time. This violates the IID assumption, and as mentionedearlier, the DeepQ agent stores and re-trains on past data (a process the authors liken to dreaming) in order to be ableto employ standard IID function learning techniques.  However, temporal order contains information that animateintelligence uses.  Information is not only contained in temporal order, but also in the fact that slow changes of thestatistics effectively create a multi-domain setting. Multi-domain data have been shown to help identify causal (and thusrobust) features, and more generally in the search for causal structure, by looking for invariances (Peters et al., 2017).This could enable RL agents to find robust components in their models that are likely to generalize to other parts ofthe state space. One way to do this is to employ model-based RL using SCMs, an approach which can help address aproblem of confounding in RL where time-varying and time-invariant unobserved confounders influence both actionsand rewards (Lu et al., 2018). In such an approach, nonstationarities would be a feature rather than a bug, and agentswould actively seek out regions that are different from the known ones in order to challenge their existing model andunderstand which components are robust. This search can be viewed and potentially analyzed as a form ofintrinsicmotivation,a concept related to latent learning in Ethology that has been gaining traction in RL (Chentanez et al., 2005).Finally, a large open area in causal learning is the connection to dynamics. While we may naively think that causality isalways about time, most existing causal models do not (and need not) talk about time. For instance, returning to ourexample of altitude and temperature, there is an underlying temporal physical process that ensures that higher placestend to be colder. On the level of microscopic equations of motion for the involved particles, there is a clear causalstructure (as described above, a differential equation specifies exactly which past values affect the current value of avariable). However, when we talk about the dependence or causality between altitude and temperature, we need notworry about the details of this temporal structure — we are given a dataset where time does not appear, and we canreason about how that dataset would look if we were to intervene on temperature or altitude. It is intriguing to thinkabout how to build bridges between these different levels of description. Some progress has been made in derivingSCMs that describe the interventional behavior of a coupled system that is in an equilibrium state and perturbed in an“adiabatic” way (Mooij et al., 2013), with generalizations to oscillatory systems (Rubenstein et al., 2018). There is nofundamental reason why simple SCMs should be derivable in general. Rather, an SCM is a high-level abstraction of anunderlying system of differential equations, and such an equation can only be derived if suitable high-level variablescan be defined (Rubenstein et al., 2017), which is probably the exception rather than the rule.RL is closer to causality research than the machine learning mainstream in that it sometimes effectively directlyestimates do-probabilities.  E.g., on-policy learning estimates do-probabilities for the interventions specified by thepolicy (note that these may not be hard interventions if the policy depends on other variables). However, as soon asoff-policy learning is considered, in particular in the batch (or observational) setting (Lange et al., 2012), issues ofcausality become

\section{Discusión}



La principal diferencia de este trabajo de investigación con los trabajos enfocados a combinar conceptos de las áreas de interés, es que la perspectiva abordada es sobre apoyar el aprendizaje de un agente de RL con información que se obtiene de un modelo causal. A pesar de las muchas suposiciones que se hacen y las limitaciones del trabajo, se puede afirmar que este trabajo es parte de los primeros intentos en incluir conceptos de RL y causalidad en un nuevo marco de trabajo. Además, aunque se presenta un método simple con un rango de aplicación muy acotado, es parte de un esfuerzo para motivar a seguir este camino que presentan de manera más formal en los trabajos mencionados. Por otro lado, la principal ventaja del trabajo presentado es que puede verse como un trabajo introductorio para comprender de manera general algunos fundamentos sobre modelos de aprendizaje basados en RL y en inferencia causal.

% Es difícil definir una línea base sobre la cuál comparar esta tesis con respecto al estado del arte. 
De acuerdo con los trabajos mencionados, se proponen diversas perspectivas que en realidad se complementan entre sí. Sin embargo, las siguientes contribuciones distinguen
la propuesta sobre los trabajos recientes:

\begin{itemize}
    \item Se utiliza un enfoque para guiar la selección de acción usando modelos gráficos causales en problemas de aprendizaje secuencial.
    \item De acuerdo con los experimentos, se muestra que incluso conocimiento causal incompleto o con ciertos errores, asiste al aprendizaje.
    \item  El método propuesto se aplica a problemas con espacios de estados continuos.
\end{itemize}


La propuesta de este documento, está en la misma dirección que los trabajos enfocados en 
utilizar instrumentos de la inferencia causal sobre algoritmos de RL. Estos, buscan dar solución a algunos problemas como son: 1) el mejorar la
selección de acciones y 2) aprender una política general. Para el primer caso, 
la selección de acciones, se lleva a cabo buscando optimizar la recompensa del agente o a través de los parámetros de un modelo causal del ambiente.
Para el segundo, se intenta aprender una política general para tareas que tienen
una estructura causal similar.
La propuesta en esta tesis es una opción más 
a estas investigaciones, que buscan
limitar la acciones de un agente
en cada paso de interacción. Esta investigación se limita a una idea simple e intuitiva como
es la consulta sobre un grafo causal para conocer cuáles son las posibles causas de un estado deseado, y por lo tanto llegar más rápido a la meta.
Una selección de acciones guiada de alguna forma, refina el espacio de 
acciones y restringe la posibilidad de cometer errores de manera recurrente.

Por otro lado, a pesar de ser un problema muy interesante,
y una de las principales motivaciones iniciales de este trabajo,
el problema del descubrimiento causal a través de interacciones con un ambiente queda fuera
de esta investigación. Algunos de los trabajos mencionados en este capítulo, brindan ideas para extender esta tesis. Entre estos, se 
marcan los diversos aspectos que se deben cubrir para
realmente afirmar que se está haciendo descubrimiento causal, como
las suposiciones en la obtención de los datos (como en \cite{zhu2019causal, dasgupta2019causal}), la definición
de lo que es un ambiente causal (como en \cite{gonzalezsoto2019von}), y 
qué elementos del modelo se aprenderán, por ejemplo, los parámetros como en \cite{lattimore2016causal}
o la estructura \cite{gonzalezsoto2020causal}.


Finalmente, como se pudo ver en la sección previa previa, 
existen diversas formas de guiar la exploración de un agente de RL, 
donde no necesariamente se lleva a cabo usando modelos causales. 
Sin embargo, es importante brindar la capacidad de aprender y/o
utilizar información causal a los agentes que atacan problemas de toma de decisiones. 
Conocer el por qué se llevó a cabo cierta secuencia de decisiones, 
es una parte importante en aplicaciones del mundo real.

Por otro lado para mostrar algunas diferencias importantes y los distintos enfoques que se han mencionado en este capítulo. La tabla \ref{table:comparison-related-work}, presenta una comparación entre los trabajos mencionados previamente y el método propuesto en esta tesis. A continuación se describe lo que indica cada una de las columnas:

\begin{itemize}
    \item \textit{Descubrimiento causal (DC):} Indica si el método propuesto aprende los parámetros del modelo causal, la estructura causal o ambos.
    \item \textit{Inferencia causal:} Denota si el trabajo utiliza un modelo causal, ya sea la estructura causal o el modelo completo.
    \item \textit{Aprendizaje por refuerzo:} Indica si el método involucra aprendizaje por refuerzo como componente principal.
    \item \textit{RL guiado:} Denota si la propuesta busca guiar la selección de acciones de un agente de RL.
    \item \textit{Determinista:} Indica si el trabajo se enfoca en tareas que no involucran incertidumbre en el ambiente.
    \item \textit{Estocástico:} Señala aquellos trabajos cuyas tareas a resolver  involucran incertidumbre, por ejemplo, las acciones ejecutadas por un agente no llevan siempre al mismo estado.
    \item \textit{Visual:} Denota si la tarea que implican observaciones que son imágenes del ambiente.
    \item \textit{Estructura causal:} Señala si el método propuesto supone que se cuenta con la estructura causal.
    \item \textit{Explicativo:} Indica si el modelo causal sirve para explicar la decisiones de un agente.
    \item \textit{Humano:} Denota si la propuesta requiere de un humano durante el aprendizaje.
\end{itemize}


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\caption{Comparación de algunos trabajos relacionados mencionados en este capítulo. Las X indican si el trabajo incluye el elemento descrito.}
\label{table:comparison-related-work}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Trabajo                                                                                                                    & DC        & IC         & RL         & \begin{tabular}[c]{@{}l@{}}RL \\ guiado\end{tabular} & Determinista & Estocástico & Visual     & \begin{tabular}[c]{@{}l@{}}Estructura \\ causal\end{tabular} & Explicativo & Humano    \\ \hline
\begin{tabular}[c]{@{}l@{}}Deconfounding reinforcement\\ learning in observational \\ settings \cite{lu2018deconfounding}\end{tabular}                & X         & X          & X          &                                                      & X            & X           & X          &                                                              &             &           \\ \hline
Playing against nature \cite{playingagainstnature2018}                                                                                                     &           & X          &            &                                                      &              & X           &            & X                                                            &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Reward tampering problems\\ and solutions in\\ reinforcement learning \cite{everitt2019reward}\end{tabular}              &           & X          & X          &                                                      & X            &             &            & X                                                            &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal discovery with \\ reinforcement learning \cite{zhu2019causal}\end{tabular}                                    & X         &            & X          &                                                      & X            & X           &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal induction from \\ visual observations \cite{nair2019causal}\end{tabular}                                       & X         & X          & X          &                                                      & X            &             & X          &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal reasoning from \\ meta-reinforcement \\ learning \cite{dasgupta2019causal}\end{tabular}                            & X         & X          & X          &                                                      &              & X           &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Explainable reinforcement \\ learning through a causal \\ lens \cite{madumal2019explainable}\end{tabular}                     &           &            & X          &                                                      & X            &             &            & X                                                            & X           &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Prioritized experience\\ replay \cite{schaul2015prioritized}\end{tabular}                                                    &           &            & X          & X                                                    & X            &             & X          &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Guided exploration in \\ deep reinforcement \\ learning \cite{mazumder2019guided}\end{tabular}                            &           &            & X          & X                                                    & X            &             &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Trial without Error:\\ Towards Safe Reinforcement\\ Learning via \\ Human Intervention \cite{saunders2017trial}\end{tabular} &           &            & X          & X                                                    & X            &             &            &                                                              &             & X         \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal bandits: learning \\ good interventions \\ via causal inference \cite{lattimore2016causal}\end{tabular}             & X         & X          &            &                                                      &              &             &            &                                                              &             &           \\ \hline
 Goal-based action priors \cite{Abel2015GoalBasedAP}             &          &      X   &    X        &  X                                                    &           X   &     X        &            &                                                              &     X        &      X     \\ \hline
\begin{tabular}[c]{@{}l@{}}Overcoming exploration in RL\\ with demostrations \cite{lattimore2016causal}\end{tabular}             &          &           &  X          & X                                                     &              &             &            &                                                              &             &     X      \\ \hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Método propuesto}                                                                                                  & \textbf{} & \textbf{X} & \textbf{X} & \textbf{X}                                           & \textbf{X}   & \textbf{X}  & \textbf{X} & \textbf{X}                                                   & \textbf{}   & \textbf{} \\ \hline
\end{tabular}%
}
\end{table}