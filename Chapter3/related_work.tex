\chapter{Trabajo relacionado}\label{chapter3}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter3/Figs/}}


El aprendizaje por refuerzo y la inferencia
causal han evolucionado de manera 
independiente y prácticamente sin 
interacción alguna. Sin embargo, ambos campos
pueden estar relacionados en algunos
procesos de solución de problemas.
Por lo tanto, trabajos recientes
se han enfocado en conectar estas dos
áreas \cite{Gershman2017, 6-DBLP:journals/midm/YuDLR19, lu2018deconfounding, dasgupta2019causal}. La meta de estos trabajos es mostrar que el RL puede ser más robusto y general a través de mecanismos
causales y viceversa.
Este enfoque, que busca unir ambas áreas, es conocido como
\textit{aprendizaje por refuerzo causal}. Este paradigma combina
ambas perspectivas para resolver problemas que no pueden
resolverse de manera individual en cada disciplina \cite{CausalRL2019EliasB, chaochao_2019}.
Hasta donde sabe el autor de esta investigación, este nuevo enfoque propuesto no ha sido sido formulado de tal manera que se sustente formalmente.
Sin embargo, los esfuerzos que se han hecho desde esta perspectiva, han abierto
nuevos problemas y parece ser el camino correcto para solucionar algunos otros
que están siendo estudiados.
% ni el problema de aprendizaje por 
% refuerzo ni el del enfoque de causalidad en este trabajo, han 


Se pueden dividir el aprendizaje por refuerzo causal
en dos áreas. La primera busca usar razonamiento causal en aprendizaje por refuerzo y la segunda que tiene como objetivo descubrir relaciones causales utilizando aprendizaje por refuerzo. En las siguientes dos secciones se describen
algunos trabajos de los enfoques en ambas direcciones. Algunas de las
propuestas descritas no están del todo alineadas con esta investigación, sin embargo, es un área que recientemente está siendo explorada y los resultados de éstas parecen prometedores.
Finalmente, en la última sección se mencionan algunas propuestas que, de manera similar a este trabajo, buscan guiar la selección de acciones en un agente de RL. No obstante, la principal diferencia con este trabajo estos es que los métodos expuestos no utilizan información causal.

% This chapter should include a comparison of your work
% with closely related efforts. It should demonstrate the principal differences and
% similarities with respect to (1) the details of the problem, (2) the approach, (3) the
% results, and possibly (4) the methodology. In particular, when you compare your
% approach with those of others, it is important that you objectively weigh the advantages
% and disadvantages. This increases your credibility and that of your study. Further,
% it will show that you did your homework, and that you know what the state-of-the-art
% in the area is. This will support and strengthen any claims of originality in your work.

% DEbo hacer una lista de los artículos que más me interesan


\section{Usando razonamiento causal en aprendizaje por refuerzo}


Los autores en \cite{Gershman2017}, desde un enfoque psicológico, establecen que el modelo usado en los algoritmos de aprendizaje por refuerzo basados en modelo,
es causal.
Ellos describen algunas relaciones causales fundamentales en el aprendizaje por refuerzo. Tomar una acción en un estado causa una recompensa y una transición a un nuevo estado.
Éstas relaciones se resumen en el Cuadro \ref{table:causal-relationships}. 

\begin{table}[h]
\centering
\caption{Relaciones causales en RL según \cite{Gershman2017}.}
\label{table:causal-relationships}
\begin{tabular}{@{}ll@{}}
\toprule
Causas         & Efecto     \\ \midrule
estado, acción & estado     \\
estado, acción & recompensa \\ \bottomrule
\end{tabular}
\end{table}

A pesar de que esta perspectiva tiene sentido y es natural para un ser humano, no se aborda o explica el mecanismo intervencionista de la inferencia causal. 
Por otra parte, en esta tesis, más que plantear un algoritmo de RL 
basado en modelo en términos de causales, se
busca complementar el aprendizaje por refuerzo con información causal.
Esta información, a diferencia de lo propuesto por \cite{Gershman2017}, está
basada en relaciones entre variables de acción y otras que no necesariamente son la informacióń directa que envía el ambiente a un agente.

% Los autores en \cite{lu2018deconfounding} presentan una método formulado para resolver problemas de RL con datos observacionales.

% Los autores
% presentan un grafo causal donde reemplazan a los estados 
% por variables latentes de los estados en un espacio de menor dimensión. Además, añaden factores
% de confusión no observables (aquellas variables que afectan a la acción y la salida). Sin embargo, ya que el objetivo de ese trabajo es manejar los factores de confusión en problemas con datos observacionales algunos elementos causales 
% quedan implícitos. Por ejemplo, los parámetros del modelo causal quedan codificados en aproximadores de funciones y no es claro si los patrones aprendidos tienen una perspectiva más asociativa que causal. En este trabajo de tesis, 
% se ataca un problema más pequeño y sin intentar construir un puente entre el 
% problema completo de RL y la causalidad. Pero se toma la idea de trabajar con
% relaciones causales entre variables de acción y variables que representen 
% estados en un espacio menor.

Los autores en \cite{lu2018deconfounding} presentan un esquema de relaciones causales similares al del Cuadro \ref{table:causal-relationships} a lo largo del tiempo donde atacan el problema de aprender buenas políticas a partir de datos observacionales en la cual factores no observados afectan las acciones observadas y recompensas \cite{pearl_2009}. El método propuesto consiste de manera general en dos pasos: aprender un modelo generador \cite{jebara2012machine} sobre un espacio de variables latentes, es decir, las observaciones son mapeadas a un espacio de menor dimensión.  Este modelo de variables latentes aprendido a partir de datos observacionales permite descubrir los factores ocultos e inferir como afectar las acciones y recompensas. Entonces, se aprende una política basándose en este modelo. 
% Se puede explotar el modelo aprendido para generar trayectorias para evaluación de políticas y optimización.  
% Se estima la función de transición con el modelo generador y se calcular la recompensa. Con  se generan trayectoria con el la función estimada para entrenar la política. 
El método de RL que utiliza el modelo que aprende los factores ocultos se desempeña mejor que aquel sin el cálculo de estos. La principal conexión que se hace entre aprendizaje por refuerzo y causalidad es calcular las recompensas en el modelo generador aprendido usando el cálculo do. A pesar de que este trabajo propone aprovechar información causal para calcular las recompensas, está enfocado en tareas con datos observacionales. Por otra parte, en esta tesis se propone auxiliar el aprendizaje con conocimiento previo sobre una tarea para un agente que interactúa con un ambiente directamente.
% % consider the problem of learning goodpolicies solely from historical data in which unobserved factors (confounders) affect bothobserved actions and rewards.
% También se han atacado problemas clásicos de RL 
% en configuraciones observacionales donde existen factores de confusión. Los autores en \cite{bareinboim2015bandits, lu2018deconfounding}
% han abordado el manejo de estos factores de confusión .

La idea de utilizar conocimiento de modelos
causales para evitar o reducir el aprendizaje a 
prueba y error en RL es un área con escasa 
exploración pero prometedora.
Los autores en \cite{lattimore2016causal},
explotan información causal en el problema
del bandido y muestran de qué manera, a través
de intervenciones, se puede mejorar la
tasa en la que se identifican las acciones
con una recompensa más alta. A pesar de atacar un problema más pequeño
que el que se plantea en un MDP, su propuesta motiva a utilizar 
conocimiento causal para tomar decisiones.

Desde un enfoque hacia la teoría de decisiones, los autores en \cite{playingagainstnature2018, gonzalezsoto2019von} proponen una alternativa al aprendizaje por interacción que se sigue en RL en ambientes causales. Los autores platean que los métodos más utilizados en RL son meramente asociativos y que se puede aprovechar la característica de algunos ambientes gobernados por mecanismos causales. El método que proponen consiste en seleccionar la mejor acción 
	en cada interacción con respecto con los parámetros de un modelo causal. Donde estos últimos se van aprendiendo utilizando las respuestas del ambiente. De acuerdo con los resultados, su propuesta tiene un desempeño similar a un método clásico de RL, sin embargo, el primero explota la información causal del mundo. Por otra parte, una suposición que comparte con esta tesis es conocer la estructura causal de variables del ambiente. La principal diferencia es que ellos proponen una alternativa más general para la toma de decisiones bajo incertidumbre sin utilizar un enfoque por refuerzo y esta tesis se concentra en guiar la selección de acciones de un agente de RL sin involucrar descubrimiento causal.

Otro problema en RL que está siendo atacado con elementos causales es el de manipulación de la función de recompensa (reward tampering en inglés) \cite{everitt2019reward}. Descrito de manera general, este problema surge cuando un agente descubre una a estrategia que parece la mejor, pero en realidad encontró una ``laguna'' en la recompensa especificada. A veces, un agente se enfoca en recoger pequeñas recompensas y evitar aquel comportamiento que lo lleva por la recompensa mayor. Por ejemplo, un agente que debe recoger diamantes, pero existen rocas que también le dan recompensa puede inclinarse a solo buscar las rocas y no los diamantes. Los autores utilizan un diagrama de influencia causal para representar este problema. Los nodos de utilidad representan las recompensas y los nodos de decisión las acciones. Por otro lado el resto de nodos describen los estados parámetros que describen las recompensas (por ejemplo, si lo recogido es una roca o un diamante).
Dado que en esta tesis no atacamos el mismo problema, el método propuesto no modela como afectan las acciones a las recompensas ni como puede la información con respecto a las recompensas influir en la selección de una acción.

En \cite{nair2019causal}, se propone
aprender un grafo causal usando aprendizaje supervisado y
entrenar una política con ayuda del grafo y mediante aprendizaje por imitación, para generalizar sobre 
configuraciones diferentes de una misma
tarea.
El ambiente utilizado en ese trabajo es básicamente el mismo que se utiliza en esta
tesis, salvo por algunas modificaciones. Sin embargo, los problemas a atacar y los enfoques
de solución son diferentes. Ellos aprenden el grafo con aprendizaje supervisado y
aprenden una política con aprendizaje por imitación. En esta tesis se supone que
el modelo causal es dado y que es información extra que sirve como apoyo a un agente de RL.


Por otra parte existe la pregunta de si es posible para los modelos modernos de aprendizaje por refuerzo adquirir conocimiento causal similar al humano. Para responder a esta interrogante,  los autores en \cite{edmonds2018human} comparan el desempeño de humanos y de una algoritmo de RL en una tarea diseñada por los autores para examinar el aprendizaje de secuencias de acciones regidas por estructuras causales.  De acuerdo con sus resultados, el algoritmo de RL es incapaz de capturar los mecanismos causales. Esto motiva a encontrar la diferencia entre el aprendizaje causal humano y el RL. Por otro lado, de los resultados que se obtuvieron en esta tesis se puede intuir lo que se ve en ese trabajo. A veces, tener el modelo causal que gobierna al ambiente es suficiente sin necesidad de un aprendizaje basado en la optimización de una recompensa.

En general, a diferencia de los trabajos anteriores,
en esta investigación se busca reducir el espacio de exploración
de acciones. 
Por medio del uso de un modelo causal, la selección de acciones es guiada, limitando así la posibilidad de cometer errores de manera recurrente.

\section{Descubrimiento causal usando aprendizaje por refuerzo}

El descubrir la estructura causal entre un conjunto de variables un problema
fundamental en diferentes ciencias. A pesar de que 
el alcance de esta tesis no cubre la parte de descubrimiento causal 
a continuación se mencionan brevemente algunos trabajos 
que han logrado mostrar que el razonamiento causal puede surgir a partir del aprendizaje por refuerzo \cite{dasgupta2019causal, madumal2019explainable, zhu2019causal}. 


Existe el problema de descubrir la estructura causal de un conjunto de variables. El trabajo  de \cite{zhu2019causal} propone utilizar aprendizaje por refuerzo para buscar un grafo acíclico dirigido con el puntaje más alto. En vez de que la meta sea aprender una política, se usa el RL como estrategia de búsqueda donde la salida final es una grafo. La búsqueda es sobre todos los grafos generados durante el entrenamiento con la mejor recompensa. La recompensa está diseñada para incorporar una función de puntaje y dos términos de penalidad para forzar que no existan ciclos en el grafo.

Además de descubrir una estructura causal a partir de datos, también se busca estimar efectos causales usando el modelo conocido. Los autores en \cite{dasgupta2019causal} proponen abarcar ambas tareas.
La idea general es mostrar que el razonamiento causal puede surgir de RL.
Se propone entrenar un agente que haga razonamiento causal a través del uso de meta
aprendizaje por refuerzo. Se entrena una red recurrente (RNN) con RL para resolver
una serie de problemas que contienen estructuras causales. Se encontró que el agente
puede realizar razonamiento causal en situaciones no vistas de manera que se obtengan
recompensas. El agente puede seleccionar intervenciones informativas, hacer inferencia
causal a partir de datos observacionales y hacer predicciones contrafactuales. Con respecto a la tesis, este método difiere principalmente porque se enfoca en mostrar que el razonamiento causal puede surgir de aprendizaje por refuerzo sobre datos sintéticos y no en la dirección de usar datos para apoyar el aprendizaje de RL.

% En \cite{bengio2019metatransfer} se propone un aprender estructuras 
% causales basándose en qué tan rápido se adapta un agente a nuevas
% distribuciones. Esto, bajo la suposición de que cuando el conocimiento de una distribución está representado apropiadamente, entonces los cambios son menores.

Otro trabajo enfocado a aprovechar aprender y explotar un modelo causal para brindar a un agente la capacidad de hacer razonamiento causal y completar tareas dirigidas por metas en ambientes donde las observaciones son imágenes se presenta en \cite{nair2019causal}. Los autores proponen atacar el problema en dos fases, una de inducción causal, donde el agente descubre las relaciones de causa y efecto a través de la realización de acciones y la observación de las salidas y la segunda fase de inferencia causa donde el agente utiliza las relaciones causales adquiridas para guiar sus acciones para completar una tarea. Se propone atacarlo como un problema de meta aprendizaje. Primero, se aprende de manera supervisada un modelo de inducción causal que recibe como entrada una trayectoria de datos observacionales y la salida es un grafo que captura la estructura causal. Después, la estructura causal es utilizada para aprender la política condicionada a la meta, la cual aprende a utilizar el modelo para completar la meta específica.	Aunque el aprendizaje de modelo que produce la estructura causal parece meramente asociativo, la naturaleza de la tarea condicionada a metas sobre ambientes gobernados por un modelo causal inspiró parte de los experimentos de esta tesis. En específico, se utiliza el mismo ambiente (interruptores de luz) para evaluar el método que se propone. La principal diferencia es que en esta tesis se acota a solo la fase de aprovechar la estructura causal para guiar a una agente y aprender una función de valor de acción. Por otro lado, este trabajo se concentra en utilizar modelos de atención para aprender una política dado el grafo y usando aprendizaje por imitación.


Además de los enfoques de descubrimiento causal mencionados también existe la tarea de hacer sistemas inteligentes más transparente, interpretables y explicables. Algunos trabajos como de \cite{shi2020selfsupervised, madumal2019explainable} se concentran en la explicación, una justificación las decisiones y acciones que toma el sistema. Toman como base la idea de que las personas ven al mundo a través de lentes causales, construyen relaciones causales para actuar en el mundo y entender nuevos eventos y explicar eventos. Por lo que en \cite{madumal2019explainable} proponen codificar un modelo causal entre variables de interés. introducen el modelo de influencia de acción para agentes de RL. Este modelo de influencia de acción se aproxima al modelo causal del ambiente relativo a las acciones que toma el agente. De manera muy general, su modelo consiste de un conjunto de variables que describen el estado del ambiente donde cada variable tiene un conjunto de ecuaciones estructurales que describen el efecto de una acción y otras variables de estado sobre ellas. Los autores proponen que se provea un DAG donde los nodos representan variables de estado y las aristas las acciones. Las ecuaciones que corresponden a cada acción sobre cada variable se aprende con modelos de regresión usando datos que se van obteniendo a partir de experiencias. Los autores evalúan los resultados usando humanos que califican qué tan bien los modelos aprendidos explican las acciones de los agentes. La principal diferencia con el trabajo de esta tesis es que ellos se concentran en aprender las ecuaciones estructurales para hacer consultas posteriores al aprendizaje. Por otra parte, se comparte la misma debilidad que es la de contar con el modelo causal de antemano.
% ausal reasoning has been an indispensable capability for humans and other in-telligent animals to interact with the physical world. In this work, we propose toendow an artificial agent with the capability of causal reasoning for completinggoal-directed tasks.  We develop learning-based approaches to inducing causalknowledge in the form of directed acyclic graphs, which can be used to contex-tualize a learned goal-conditional policy to perform tasks in novel environmentswith latent causal structures.  We leverage attention mechanisms in our causalinduction model and goal-conditional policy, enabling us to incrementally generatethe causal graph from the agent’s visual observations and to selectively use theinduced graph for determining actions.  Our experiments show that our methodeffectively generalizes towards completing new tasks in novel environments withpreviously unseen causal structures


\section{Exploración guiada en aprendizaje por refuerzo}

Un problema fundamental en los algoritmos de aprendizaje por refuerzo
es el balance entre la exploración del ambiente y la explotación
de la información con la que cuenta el agente y existen diversas técnicas para lidiar con este compromiso.
Las estrategias de explotación y exploración no 
son dirigidas y no buscan explícitamente transiciones
interesantes \cite{mcfarlane2018survey}.
Sin embargo, usar modelos de predicción parece una manera 
prometedora para lidiar con este problema \cite{hafner2019dream}.

 
Una técnica reciente para una exploración 
guiada, sin apoyo de un modelo causal, ha sido
propuesta por \cite{mazumder2019guided}.
Los autores proponen un nuevo método
para acelerar el entrenamiento de algoritmos de RL con el uso de una propiedad presente
en algunos problemas, la cual denominan \textit{permisibilidad estado-acción}.
La idea principal es tener un clasificador
que guíe el paso de la selección de acciones. Éste clasifica si se llega
a una solución óptima dada la acción y el
estado actual. Sin embargo, el enfoque sigue siendo
asociativo.

Los autores en \cite{saunders2017trial} proponen un esquema 
para exploración segura a través de la intervención de un humano. El humano controla la interfaz entre el agente y el ambiente, vigila
al agente y bloquea cualquier acción que pueda ser catastrófica. Este enfoque
está limitado a donde un humano pueda intervenir durante la interacción 
del agente con su ambiente.

En el aprendizaje por refuerzo profundo existe el problema de que las experiencias del búfer de repetición son muestreadas de manera uniforme sin tomar en cuenta su importancia. Los autores en \cite{schaul2015prioritized}  se enfocan en priorizar aquellas transiciones por lo que hacen que las experiencias sean más eficientes y efectivas que muestrear de manera aleatoria. El usar un búfer de experiencias conduce a dos opciones de diseño en dos niveles: el primero con respecto a qué experiencias almacenar y el segundo a cuáles experiencias utilizar. Los autores en \cite{schaul2015prioritized} se enfocan en la segunda opción mientras que la tesis en la primera. En esta tesis se hace la suposición que guardar mejores experiencias también conduce a un aprendizaje con un mejor desempeño.

% Hablar de los world model
% he program to move statistical learning towards causal learning has links to reinforcementlearning (RL), a sub-field of machine learning. RL used to be (and still often is) considered a field that has trouble withreal-world high-dimensional data, one reason being that feedback in the form of a reinforcement signal is relativelysparse when compared to label information in supervised learning. The DeepQ agent (Mnih et al., 2015) yielded resultsthat the community would not have considered possible at the time, yet it still has major weaknesses when compared toanimate intelligence. Two major issues can be stated in terms of questions (Schölkopf, 2015, 2017):Question 1: why is RL on the original high-dimensional ATARI games harder than on downsampled versions?Forhumans, reducing the resolution of a game screen would make the problem harder, yet this is exactly what was done tomake the DeepQ system work. Animals likely have methods to identify objects (in computer game lingo, “sprites”) bygrouping pixels according to “common fate” (known from Gestalt psychology) or common response to intervention.This question thus is related to the question of what constitutes an object, which concerns not only perception butalso concerns how we interact with the world. We can pick up one object, but not half an object. Objects thus alsocorrespond to modular structures that can be separately intervened upon or manipulated.  The idea that objects aredefined by their behavior under transformation is a profound one not only in psychology, but also in mathematics, cf.Klein (1872); MacLane (1971)


% Question 2: why is RL easier if we permute the replayed data?As an agent moves about in the world, it influences thekind of data it gets to see, and thus the statistics change over time. This violates the IID assumption, and as mentionedearlier, the DeepQ agent stores and re-trains on past data (a process the authors liken to dreaming) in order to be ableto employ standard IID function learning techniques.  However, temporal order contains information that animateintelligence uses.  Information is not only contained in temporal order, but also in the fact that slow changes of thestatistics effectively create a multi-domain setting. Multi-domain data have been shown to help identify causal (and thusrobust) features, and more generally in the search for causal structure, by looking for invariances (Peters et al., 2017).This could enable RL agents to find robust components in their models that are likely to generalize to other parts ofthe state space. One way to do this is to employ model-based RL using SCMs, an approach which can help address aproblem of confounding in RL where time-varying and time-invariant unobserved confounders influence both actionsand rewards (Lu et al., 2018). In such an approach, nonstationarities would be a feature rather than a bug, and agentswould actively seek out regions that are different from the known ones in order to challenge their existing model andunderstand which components are robust. This search can be viewed and potentially analyzed as a form ofintrinsicmotivation,a concept related to latent learning in Ethology that has been gaining traction in RL (Chentanez et al., 2005).Finally, a large open area in causal learning is the connection to dynamics. While we may naively think that causality isalways about time, most existing causal models do not (and need not) talk about time. For instance, returning to ourexample of altitude and temperature, there is an underlying temporal physical process that ensures that higher placestend to be colder. On the level of microscopic equations of motion for the involved particles, there is a clear causalstructure (as described above, a differential equation specifies exactly which past values affect the current value of avariable). However, when we talk about the dependence or causality between altitude and temperature, we need notworry about the details of this temporal structure — we are given a dataset where time does not appear, and we canreason about how that dataset would look if we were to intervene on temperature or altitude. It is intriguing to thinkabout how to build bridges between these different levels of description. Some progress has been made in derivingSCMs that describe the interventional behavior of a coupled system that is in an equilibrium state and perturbed in an“adiabatic” way (Mooij et al., 2013), with generalizations to oscillatory systems (Rubenstein et al., 2018). There is nofundamental reason why simple SCMs should be derivable in general. Rather, an SCM is a high-level abstraction of anunderlying system of differential equations, and such an equation can only be derived if suitable high-level variablescan be defined (Rubenstein et al., 2017), which is probably the exception rather than the rule.RL is closer to causality research than the machine learning mainstream in that it sometimes effectively directlyestimates do-probabilities.  E.g., on-policy learning estimates do-probabilities for the interventions specified by thepolicy (note that these may not be hard interventions if the policy depends on other variables). However, as soon asoff-policy learning is considered, in particular in the batch (or observational) setting (Lange et al., 2012), issues ofcausality become

\section{Discusión}

La principal diferencia del trabajo de investigación con los trabajos enfocados a combinar conceptos de las áreas de interés es que la perspectiva abordada es sobre apoyar el aprendizaje de un agente de RL con información que se obtiene de un modelo causal. A pesar de las muchas suposiciones que se hacen y las limitaciones del trabajo, se puede afirmar que este trabajo es parte de los primeros intentos en incluir conceptos de RL y causalidad en un nuevo marco de trabajo. Además, aunque se presenta un método simple con un rango de aplicación muy acotado, es parte de un esfuerzo para motivar a seguir este camino que presentan de manera más formal en los trabajos mencionados. Por otro lado, la principal ventaja del trabajo presentado es que puede verse como un trabajo introductorio para comprender de manera general algunos fundamentos sobre modelos de aprendizaje basados en RL y en inferencia causal.

En el Cuadro \ref{table:comparison-related-work}, se presenta una tabla comparativa entre los trabajos mencionados previamente y el método propuesto en esta tesis. A continuación se describe lo que indica cada una de las columnas:

\begin{itemize}
    \item \textit{Descubrimiento causal (DC):} Indica si el método propuesto aprende los parámetros del modelo causal, la estructura causal o ambos.
    \item \textit{Inferencia causal (IC):} Denota si el trabajo utiliza un modelo causal, ya sea la estructura causal o el modelo completo.
    \item \textit{Aprendizaje por refuerzo (RL):} Indica si el método involucra aprendizaje por refuerzo como componente principal.
    \item \textit{RL guiado:} Denota si la propuesta busca guiar la selección de acciones de un agente de RL.
    \item \textit{Determinista:} Indica si el trabajo se enfoca en tareas que no involucran incertidumbre en el ambiente.
    \item \textit{Estocástico:} Señala aquellos trabajos cuyas tareas a resolver  involucran incertidumbre, por ejemplo, las acciones ejecutadas por un agente no llevan siempre al mismo estado.
    \item \textit{Visual:} Denota si la tarea que implican observaciones que son imágenes del ambiente.
    \item \textit{Estructura causal:} Señala si el método propuesto supone que se cuenta con la estructura causal.
    \item \textit{Explicativo:} Indica si el modelo causal sirve para explicar la decisiones de un agente.
    \item \textit{Humano:} Denota si la propuesta requiere de un humano durante el aprendizaje.
\end{itemize}


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\caption{Tabla comparativa de algunos trabajos relacionados mencionados en este capítulo. Las X indican si el trabajo incluye el elemento descrito.}
\label{table:comparison-related-work}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Trabajo                                                                                                                    & DC        & IC         & RL         & \begin{tabular}[c]{@{}l@{}}RL \\ guiado\end{tabular} & Determinista & Estocástico & Visual     & \begin{tabular}[c]{@{}l@{}}Estructura \\ causal\end{tabular} & Explicativo & Humano    \\ \hline
\begin{tabular}[c]{@{}l@{}}Deconfounding reinforcement\\ learning in observational \\ settings \cite{lu2018deconfounding}\end{tabular}                & X         & X          & X          &                                                      & X            & X           & X          &                                                              &             &           \\ \hline
Playing against nature \cite{playingagainstnature2018}                                                                                                     &           & X          &            &                                                      &              & X           &            & X                                                            &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Reward tampering problems\\ and solutions in\\ reinforcement learning \cite{everitt2019reward}\end{tabular}              &           & X          & X          &                                                      & X            &             &            & X                                                            &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal discovery with \\ reinforcement learning \cite{zhu2019causal}\end{tabular}                                    & X         &            & X          &                                                      & X            & X           &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal induction from \\ visual observations \cite{nair2019causal}\end{tabular}                                       & X         & X          & X          &                                                      & X            &             & X          &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal reasoning from \\ meta-reinforcement \\ learning \cite{dasgupta2019causal}\end{tabular}                            & X         & X          & X          &                                                      &              & X           &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Explainable reinforcement \\ learning through a causal \\ lens \cite{madumal2019explainable}\end{tabular}                     &           &            & X          &                                                      & X            &             &            & X                                                            & X           &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Prioritized experience\\ replay \cite{schaul2015prioritized}\end{tabular}                                                    &           &            & X          & X                                                    & X            &             & X          &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Guided exploration in \\ deep reinforcement \\ learning \cite{mazumder2019guided}\end{tabular}                            &           &            & X          & X                                                    & X            &             &            &                                                              &             &           \\ \hline
\begin{tabular}[c]{@{}l@{}}Trial without Error:\\ Towards Safe Reinforcement\\ Learning via \\ Human Intervention \cite{saunders2017trial}\end{tabular} &           &            & X          & X                                                    & X            &             &            &                                                              &             & X         \\ \hline
\begin{tabular}[c]{@{}l@{}}Causal bandits: learning \\ good interventions \\ via causal inference \cite{lattimore2016causal}\end{tabular}             & X         & X          &            &                                                      &              &             &            &                                                              &             &           \\ \hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Método propuesto}                                                                                                  & \textbf{} & \textbf{X} & \textbf{X} & \textbf{X}                                           & \textbf{X}   & \textbf{X}  & \textbf{X} & \textbf{X}                                                   & \textbf{}   & \textbf{} \\ \hline
\end{tabular}%
}
\end{table}