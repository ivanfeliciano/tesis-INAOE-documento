\chapter{Experimentos y resultados}\label{chapter5}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter5/Figs/}}


En este capítulo se describe la experimentación y los resultados obtenidos
que muestran la mejora en el desempeño de un agente que aprende
con y sin información adicional de un grafo causal.
Para probar el concepto propuesto se atacan dos problemas: la tarea de clásica del taxi \cite{Dietterich:2000:HRL:1622262.1622268} y la de los
interruptores de luz propuesta por \cite{nair2019causal}.
En resumen, los experimentos consisten en integrar el grafo $\mathcal{D}$ a la política $\epsilon$-greedy
en el algoritmo $Q$-learning \cite{watkins1992q}.
En la política $\epsilon$-greedy en vez de mantener fijo a $\epsilon$, se propone empezar motivando al agente a explorar y usar
el modelo causal e ir disminuyendo $\epsilon$ para dar más peso a la explotación.
Se comparan cuatro algoritmos, \textit{Q-learning sin información
adicional}, \textit{Q-learning con una estructural causal completa}, \textit{Q-learning con una estructura parcial} y un \textit{Q-learning con una estructura incorrecta}.
Dependiendo de la configuración experimental, cada uno de los algoritmos se ejecuta en una versión determinista y otra estocástica del ambiente. 
En las siguientes secciones se describen a detalle los experimentos realizados y los resultados. Todo el software desarrollado está 
disponible en \url{https://github.com/ivanfeliciano/causal_rl/}.


\section{Problema del taxi}\label{sec:exp-taxi}

En esta sección se muestran los experimentos y resultados del método propuesto para el problema clásico del taxi \cite{Dietterich:2000:HRL:1622262.1622268}.
Se realiza un par de experimentos sobre dos configuraciones del ambiente diferentes,
una determinista y otra estocástica. De acuerdo con los resultados, 
el algoritmo usando un grafo causal como ``oráculo'' para seleccionar sus acciones
resuelve la tarea más rápido (alcanza una racha de episodios con una recompensa ``óptima'' definida en este trabajo) en el ambiente determinista. Sin embargo, en el 
caso del ambiente estocástico, no se tiene evidencia suficiente para concluir que un
método es mejor que otro.


\subsection{Descripción de la tarea}

El primer problema a resolver es la tarea clásica del taxi \cite{Dietterich:2000:HRL:1622262.1622268}.
La Figura \ref{fig:taxi} muestra gráficamente el problema.
Existen cuatro posiciones en el mundo marcadas como R, B, G, y Y. 
La tarea es episódica y en cada episodio, 
el taxi comienza en un cuadro aleatoriamente elegido. 
Existe un pasajero en una de la cuatro posiciones (también elegida
aleatoriamente), y el pasajero desea ser transportado a una de las
cuatro zonas.
El taxi debe dirigirse a la posición del pasajero, recogerlo, ir a su destino y dejarlo.
El episodio termina cuando el pasajero es dejado en su destino.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{Chapter5/Figs/taxi-env.pdf}
    \caption{La cuadrícula del ambiente del taxi. El taxi se encuentra en el cuadro central. En esta configuración del mundo, el objetivo es recoger al pasajero en la posición R y llevarlo a la posición G. Las posiciones alternativas de inicio o meta  del taxi
    se representan con las letras Y y B.}
    \label{fig:taxi}
\end{figure}

El conjunto de acciones $\mathcal{A}$ está compuesto por seis elementos: una acción para recoger $a_1$, una para dejar $a_2$ y
cuatro acciones de 
navegación que trasladan al taxi un cuadro al norte, sur, 
este u oeste, denotadas por $a_3, \dots, a_6$, respectivamente.
Existe una recompensa de -1 por cada acción, una recompensa adicional de 20 por cada pasajero llevado a su destino 
exitosamente y una penalización de -10 por acciones ilegales
de recolección y dejado.
El espacio de estados $\mathcal{S}$ tiene como elementos 
500 tuplas de tres elementos donde describen los 25 cuadros, las 5 posiciones del pasajero (incluyendo cuando está en el taxi) y los 4 destinos.


% Aquí ahondar más sobre el conjunto X y G, quienes lo compoenen y como está de pequeñito el problema
El conjunto $\mathcal{X}$ contiene 4 variables que traducen las tuplas con las posiciones del taxi y del pasajero en variables binarias. $x_1$
es la variable que dice si el taxi está en la misma posición que el
pasajero, $x_2$ es la variable que denota si el pasajero es llevado dentro del taxi, $x_3$ describe si el taxi está en la posición destino,
y $x_4$ es la variable que representa al estado de que el pasajero es entregado correctamente. Es una tarea relativamente simple y $x_1 \neq x_3$, por lo tanto las metas son $\mathcal{G} = \{\mathbf{g_1}, \mathbf{g_2}\}$, donde $\mathbf{g_1} = [1, 1, 0 , 0]$ y $\mathbf{g_2} = [1,1,1,1]$. El primer vector se puede ver como el sub objetivo de que el pasajero aborda el taxi y el segundo vector es la meta general, 
entregar al pasajero en su destino.
El grafo causal $\mathcal{D}$ entre las variables de acción y los estados se puede 
ver en la Figura \ref{fig:cm-taxi}. Para este problema, los efectos
necesitan de todas sus causas para suceder, por ejemplo, para que el
pasajero esté dentro del taxi, $x_2 = 1$, entonces se debe actuar
subiéndolo al vehículo $a_1$ y además estar en la misma ubicación que
el pasajero.

\begin{figure}
    \centering
    \includegraphics[scale=0.3]{Chapter5/Figs/taxi_cg.pdf}
    \caption{Propuesta de una estructura causal para la tarea del taxi. Los nodos azules
    denotan a las acciones recoger y dejar, $a_1$ y $a_2$ respectivamente. Los nodos verdes son las variables que describen algunos estados del ambiente; $x_1$
es la variable que indica si el taxi está en la misma posición que el
pasajero, $x_2$  si el pasajero es llevado dentro del taxi, $x_3$ corresponde a si el taxi está en la posición destino y $x_4$ que indica que el pasajero es entregado correctamente. Las cuatro acciones que corresponden a los movimientos
se encuentran como nodos aislados.}
    \label{fig:cm-taxi}
\end{figure}


\subsection{Configuración experimental}

Debido al diseño de la tarea por \citet{Dietterich:2000:HRL:1622262.1622268}, el tamaño de $\mathcal{D}$, e 
implementación por \citet{gym2016brockman},
para este problema no se ahonda en realizar experimentos con diferentes configuraciones.
Sólo se comparan dos algoritmos,
el algoritmo Q-learning con y sin la estructura causal, donde
se busca aprender la política (a través de la función de valor de acción $Q$) que guíe el comportamiento del agente
en cada estado.
Además, los dos algoritmos se prueban sobre dos versiones del ambiente, una determinista
y otra estocástica. 
La versión determinista corresponde a aquella donde
el agente ejecuta satisfactoriamente la acción que ha seleccionado. 
En el ambiente estocástico, intentar cierta acción no implica 
que esta se lleve a cabo exitosamente. Este comportamiento
se simula asignando una probabilidad alta a ejecutar correctamente
la acción seleccionada y 
con una probabilidad baja, repartida uniformemente sobre el
resto de acciones, actúa de otra forma, por ejemplo,
puede que al intentar dejar el pasajero, el taxi se mueva
hacia el norte.
Los valores de los parámetros para los experimentos se muestran en la tabla \ref{tab:tax-params}. Se fija una tasa de aprendizaje $\alpha$ para dar más peso al TD-error. Se ejecutan $M$ simulaciones y cada una consiste en ejecutar cada algoritmo $k$  episodios.  El valor de $\epsilon$  se va actualizando con respecto a la regla $\epsilon = \max(\epsilon_{\min}, \epsilon_{\max} - \frac{|\epsilon_{\max} - \epsilon_{\min}|}{H \times k \times \delta} \times t)$. Donde, de acuerdo con las especificaciones e implementación del ambiente por \citet{gym2016brockman}, $H = 200$. Además, se opta por que se alcance el mínimo valor de $\epsilon_{\min}$ un poco antes del primer cuarto del ciclo de entrenamiento, haciendo $\delta = 0.2$. Para el caso donde se hace frente a un ambiente estocástico, la probabilidad de ejecutar la acción que se seleccionó es $p_a = 0.8$ mientras que la probabilidad de realizar cualquiera de las otras acciones se distribuye uniformemente sobre el complemento. Finalmente, como medida
de desempeño se usa la recompensa promedio por episodio.

Para definir el número de simulaciones necesarias, y llevar a cabo las pruebas 
estadísticas; se exige un poder estadístico
de $0.8$, un tamaño de efecto de $0.5$ (un efecto medio según la escala
de \citet{cohen2013statistical}), y un nivel de significancia de $0.05$. Con estos parámetros se calculó el valor de $M$ (usando la biblioteca desarrollada por \citet{seabold2010statsmodels}). De acuerdo con los resultados, son necesarias al menos 45 simulaciones, por lo que se asigna $M = 50$. 

\begin{table}[!h]
\centering
\caption{Parámetros para los algoritmos Q-learning y Q-learning con la estructura causal.}
\label{tab:tax-params}
\begin{tabular}{@{}llllllllll@{}}
\toprule
Parámetro & Tasa de aprendizaje $\alpha$ & $\gamma$ & $\epsilon_{\min}$ & $\epsilon_{\max}$ & $k$ & 
$M$ & $H$ & $\delta$ & $p_a$ \\ 
\midrule
Valor & $0.8$ & $0.95$ & $0.1$ & $1.0$ & 1000 & 50 & 200 & $0.2$ & $0.8$ \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Resultados}

En la Figura \ref{fig:results-taxi} se muestran los resultados obtenidos
para ambas configuraciones del ambiente (determinista y estocástico) donde
los valores de los parámetros se describen en la tabla \ref{tab:tax-params}.
La recompensa promedio por episodio para el método propuesto y para el algoritmo
original Q-learning están en color naranja y azul, respectivamente. 
Los resultados muestran que el algoritmo Q-learning guiado por el grafo da un salto inicial alto y mantiene una recompensa promedio mucho mayor hasta que ambos métodos convergen. Esto es esperado, ya que
no inicia una exploración a ciegas. En ambas versiones del ambiente, los algoritmos parecen comportarse de manera similar a partir del episodio 400.

\begin{figure}[H]
  \centering
  \subfloat[Ambiente determinista.]{\includegraphics[width=0.45\textwidth]{Chapter5/Figs/taxi_env_comparison_det_1000_50_eps_40000.pdf}\label{fig:taxi-rew-det}}
  \hfill
  \subfloat[Ambiente estocástico.]{\includegraphics[width=0.45\textwidth]{Chapter5/Figs/taxi_env_comparison_sto_1000_50_eps_20000.pdf}\label{fig:taxi-rew-sto}}
  \caption{Comparación del desempeño para los dos algoritmos en 1000 episodios. La región sombreada es la desviación estándar para 50 simulaciones.}
  \label{fig:results-taxi}
\end{figure}


La tarea para el ambiente determinista se considera resuelta cuando se obtiene una recompensa promedio de 9.7 durante 100 episodios consecutivos. Sin embargo, para fines de este experimento se propone relajar este parámetro. Se calculó la recompensa promedio en los últimos 100 episodios del algoritmo Q-learning sin información extra, la cual es 0 y -30 para el ambiente determinista y estocástico, respectivamente. Por lo tanto, para considerar la tareas resuelta y con eso medir qué algoritmo es mejor, se utilizan como valores de recompensa óptima al 0 y -30, según corresponda. 
% En la tabla \ref{tab:resultados-taxi} se muestran los resultados del número de episodios, promedio sobre $M$ experimentos,
% que tarda cada algoritmo en alcanzar la recompensa óptima propuesta durante una racha de más de 100 episodios. 
Entre menor sea el número de episodios en alcanzar la racha denota que un algoritmo aprendió más rápido.

% De acuerdo con los resultados de la tabla \ref{tab:resultados-taxi}, el algoritmo que alcanza la recompensa óptima propuesta antes, es el método Q-learning con la información causal. Esto no es del todo obvio, ya que la información extra
% podría introducir ruido al aprendizaje de la política que no necesariamente
% sería beneficioso. Sin embargo, 

Con el fin de validar si existe una diferencia en el tiempo
de aprendizaje en ambos algoritmos, se utiliza la prueba de Welch t-test como lo sugieren \citet{colas2019hitchhikers}. 
La hipótesis nula $h_0$ corresponde a que el número de episodios en alcanzar la recompensa óptima durante más de 100 episodios seguidos es igual para ambos algoritmos. 

Con la prueba estadística se busca verificar que existe una diferencia
significativa entre los episodios que tarda en alcanzar la recompensa óptima propuesta, el algoritmo usando información causal y el 
que no la tiene. Para el caso determinista, el algoritmo Q-learning
con la estructura causal resuelve la tarea en menos episodios $(\text{Media} = 544, \text{Desviación estándar} = 105.80)$ en comparación al algoritmo
Q-learning sin información $(\text{Media} = 595, \text{Desviación estándar} = 91.98)$. Los resultados de la prueba muestran una diferencia significativa,
$\text{Welch t-test} (2.526) = 96.14, p < 0.05$.


Para el caso del ambiente estocástico se falla al rechazar $h_0$, $\text{Welch t-test} (1.595) = 85.81, p > 0.05$. A pesar de que
el algoritmo con la informacióń causal $(\text{Media} = 181, \text{Desviación estándar} = 43.66)$  parece resolver la
tarea más rápido que el algoritmo Q-learning clásico $(\text{Media} = 198, \text{Desviación estándar} = 64.90)$. 

% Por lo tanto, a pesar de que los resultados en el Cuando 
% \ref{tab:resultados-taxi} indican un mejor comportamiento para el algoritmo con la estructura causal auxiliando la selección de acciones, no hay diferencia estadísticamente significativa en la versión estocástica del ambiente.

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \begin{table}[!h]
% \centering
% \caption{Resultados del número de episodios promedio y la desviación estándar en alcanzar una recompensa > 0 con una racha > 100 y > -30 con una racha > 100 para los ambientes determinista y estocástico, respectivamente.}
% \label{tab:resultados-taxi}
% \begin{tabular}{@{}lll@{}}
% \toprule
% Algoritmo & \begin{tabular}[c]{@{}l@{}}Ambiente determinista.\\ Número de episodios\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ambiente estocástico.\\ Número de episodios\end{tabular} \\ \midrule
% Q-learning & $632 \pm 69$ & $496 \pm 247$ \\
% \begin{tabular}[c]{@{}l@{}}Q-learning +\\ estructura causal\end{tabular} & $\mathbf{589 \pm 106}$ & $476 \pm 232$ \\ \bottomrule
% \end{tabular}
% \end{table}

Los resultados obtenidos son intuitivos, esto es, comenzar una exploración a ciegas es 
peor que conocer un poco del mundo por eso el valor de recompensa inicial mayor del algoritmo propuesto. 
% Por otro lado, ambos algoritmos se comportan de manera similar después de unos cientos de episodios.
Por otro lado, la información del mundo con el que cuenta el agente es muy limitada. En realidad solo se cuenta con la información de lo que provocan dos de las seis acciones posibles. Esto puede ser la razón por la que la diferencia entre los algoritmos 
en un ambiente estocástico no es significativa. Tener un modelo pequeño sumando un ambiente donde no siempre intentar una acción te lleva a un estado deseado conduce a un paso de exploración más. Esto, puede provocar que ambos algoritmos tengan tiempos de entrenamiento parecidos.
 
\section{Problema de los interruptores de luz}

En esta sección se muestran los experimentos y resultados del método propuesto para
el problema de los interruptores de luz originalmente descrito en \cite{nair2019causal}.
A diferencia del ambiente del taxi, este problema cuenta 
con diferentes variaciones. Por ejemplo,
diferentes tipos de conexiones en la estructura subyacente del problema, el tamaño de dicha estructura, etc.
En general, de acuerdo con los experimentos, el algoritmo usando un grafo causal como ``oráculo'' para seleccionar sus acciones tiene un mejor desempeño incluso en 
configuraciones donde las estructuras causales están parcialmente incorrectas o también 
al disminuir la tasa de consultas al modelo en una parte temprana del aprendizaje.

\subsection{Descripción de la tarea}\label{section:switches-example}

% En esta sección se describe un ejemplo sobre un problema en que se realizan 
% lo experimentos de esta investigación. Por ahora, solo se describe el problema
% y cómo se aplica el método propuesto.
Imagina que construyes un robot llamado Joi, con el propósito de prender o apagar las luces de las habitaciones 
de una casa de acuerdo a como se le comande.
A Joi se le puede dotar de cierto conocimiento previo de la configuración
del cableado de la casa, tal que le ayude a descifrar las correspondencias entre
interruptores de luz y los focos de las habitaciones. 
 A Joi aprende mediante prueba
 y error y recibe una señal de recompensa positiva o negativa de acuerdo a su comportamiento. Además, el robot 
 necesita repetir muchas veces la misma acción para aprender
 qué provocará realizarla. Por lo tanto, si Joi conoce los efectos de algunas de sus acciones (mover los interruptores), puede reducir sus intentos para
 resolver las conexiones entre interruptores y los focos. 
 Por ejemplo, con anticipación Joi puede saber que el mover interruptor de luz $i$ causa que
prenda o apague la luz de la cocina.
 Así que al darle la orden de prender las luces de la cocina, el baño y de alguna otra habitación, Joi solo se enfoca en aprender las partes que no
 conoce.
 
 El problema se puede ver como una tarea donde es necesario tomar las
 decisiones adecuadas para llegar a un configuración de luces deseada. Formalmente, se puede ver como un MDP condicionado a metas donde 
 
 \begin{itemize}
     \item El espacio de estados $\mathcal{S}$, es el conjunto de
     posibles observaciones de la casa desde una vista cenital como en la Figura
   \ref{fig:obs-switches}.
     \begin{figure}[H]
         \centering
         \includegraphics[scale=0.17]{Chapter4/Figs/example_obs.png}
         \caption{Observación disponible para el robot.}
         \label{fig:obs-switches}
     \end{figure}
     \item El espacio de acciones es $\mathcal{A} = \{a_1, a_2, \dots, a_N, a_{N+1}\}$
     donde cada $a_i$, con $1 \geq i \leq N$, corresponde a mover o no el interruptor de luz $i$. $a_i = 0$ si no se mueve el interruptor y $a_i = 1$ si se mueve. $a_{N+1}$ indica que no se ha movido ningún interruptor si $a_{N+1} = 1$, de otro modo es $0$.
     \item El conjunto de variables de alto nivel $\mathcal{X} = \{x_1, x_2, \dots, x_N\}$ contiene
     a las variables que describen el estado de cada foco en la casa. Por ejemplo, $x_i = 1$ establece que el foco $i$ está prendido.
     \item El grafo causal $\mathcal{D} = <\mathcal{A'}, \mathcal{X'}>$, donde
     $\mathcal{A'} \subseteq \mathcal{A}$ y $\mathcal{X'} \subseteq \mathcal{X}$, visualmente el grafo se puede representar como en la Figura \ref{fig:dag-example-switches}.
     
     \begin{figure}[H]
         \centering
         \includegraphics[scale=0.2]{Chapter4/Figs/graph.png}
        \caption{Una estructura causal posible para $N$ interruptores y $N$ luces
        con un tipo de conexión uno-a-uno.}
        \label{fig:dag-example-switches}
     \end{figure}
     \item No se cuenta con la función de transición $\mathcal{P}$, sin embargo,
     se conoce que el interruptor $i$ no necesariamente corresponde al foco $i$.
     \item El espacio de metas $\mathcal{G}$ contiene a todas las
     combinaciones posibles de focos prendidos y apagados en la casa.
     \item La recompensa inmediata $r$ se calcula a través de medir la diferencia entre la observación y la meta.
 \end{itemize}
 
 Una vez que la tarea se ha descrito como un MDP, es posible que Joi
 aprenda a alcanzar una configuración de luces deseada
 utilizando un algoritmo de RL, Q-learning por ejemplo. Además,
 dado que suponemos que Joi puede aprovechar el conocimiento que le
 ofrece el grafo causal $\mathcal{D}$, la selección de acciones se puede
 guiar usando el Algoritmo \ref{alg:guided-action-selection}. 
 En la Figura \ref{fig:example-switches} se muestra un paso en el proceso de aprendizaje.
 
 \begin{figure}
     \centering
     \includegraphics[scale=0.2]{Chapter4/Figs/example_method.png}
     \caption{Interacción del agente con su ambiente: 1) el agente observa el estado de su ambiente además de conocer el vector $\mathbf{g}$, 2) inicia
     la política de selección de acciones, 3) si el agente debe explorar, mapea la observación a un vector de variables de alto nivel $\mathbf{x}$, 4) se calculan los diferentes entre los vectores $\mathbf{x}$ y $\mathbf{g}$, 5) se consultan los padres de las variables de interés y se retorna la acción. El entrenamiento continúa su aprendizaje de manera normal.}
     \label{fig:example-switches}
 \end{figure}
 
% \subsection{Descripción de la tarea}
Para los experimentos de esta sección se ataca la tarea de control de interruptores de luz descrita de manera general arriba. Un agente tiene el control
de $N$ interruptores que controlan $N$ luces en un sitio.
Cada acción $a\in \mathcal{A}$ corresponde a mover un interruptor o 
a no mover ninguno, por lo tanto $|\mathcal{A}| = N + 1$.
El agente puede percibir dos tipos de señales del ambiente,
una imagen $s$ con una vista cenital del sitio, o vectores binarios $x \in \{0,1\}^N$ de 
macro-variables que codifican las luces prendidas, donde
$x_i = 1$ si la luz en la zona $i$ está prendida, de otro modo 
toma el valor $x_i = 0$.

Se exploran tres tipos de estructuras causales entre los
interruptores y las luces: \textit{uno-a-uno},
\textit{causa común} y \textit{efecto común}.
En los problemas con estructuras uno-a-uno cada interruptor corresponde a una sola luz.
Para el segundo tipo, de causa común, todas
las luces son controladas a lo más por un interruptor pero un
solo interruptor puede controlar más de una luz.
El tercer caso son estructuras de efecto común, donde cada interruptor
controla una sola luz, aunque múltiples interruptores
pueden controlar la misma luz. De manera visual, los tres tipos de estructuras
se muestran en la Figura \ref{fig:struct}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{Chapter5/Figs/switches_struct.png}
    \caption{Tipos de estructuras causales subyacentes posibles.}
    \label{fig:struct}
\end{figure}
La recompensa inmediata $r$ brindada al agente se calcula obteniendo la distancia entre el vector de variables de estado alto nivel $\mathbf{x}$ y el vector meta $\mathbf{g}$. En este problema se usa la distancia euclidiana para todos los tipos de estructuras causal. Incluso cuando
no parece la recompensa para todos los casos, se sigue utilizando
este enfoque debido a que así se implementó por \citet{nair2019causal}.


\subsection{Configuración experimental general}

En las siguientes secciones se compara el desempeño 
del método propuesto en diferentes escenarios, principalmente, para mostrar 
las posibilidades y ventajas de usar un información del grafo causal, completa, incompleta e incluso incorrecta. 
% A pesar de ser diferentes
% experimentos, éstos comparten la configuración de algunos elementos, por ejemplo, 
% la medida de desempeño, el número de variables $N$, etc.
Se comparan cuatro algoritmos, teniendo como base
el método Q-learning. Donde la diferencia subyace en la cantidad y calidad de la información adicional con la que cuenta. A continuación, se describen de manera breve los métodos
comparados.

\begin{itemize}
    \item \textit{Q-learning sin información adicional} ($Q_1$). Este método sirve como
    línea de base para medir que tanto mejora el aprendizaje. El algoritmo, dependiendo del espacio de estados sobre el que se trabaje, es el 
    método básico de Q-learning \cite{watkins1992q} o Q-learning profundo \cite{mnih2013playing}, para estados
    discretos y continuos, respectivamente. La selección de acciones se lleva a cabo mediante una política $\epsilon$ greedy clásica.
    \item \textit{Q-learning + estructura causal completa}($Q_2$). Durante la política de selección de acciones, el agente cuenta con la estructura causal del ambiente completa y verdadera $\mathcal{D}$.
    \item \textit{Q-learning + estructura causal incompleta}($Q_3$). En este caso, el agente cuenta con un subgrafo $\mathcal{D'}$ del grafo $\mathcal{D}$. Este subgrafo se genera eliminando aristas de $\mathcal{D}$ aleatoriamente.
    \item \textit{Q-learning + estructura causal incorrecta}($Q_4$). Este algoritmo consulta un modelo $\mathcal{D}''$ con relaciones espurias y sin algunas relaciones verdaderas. Este grafo $\mathcal{D}''$ se obtiene generando un subgrafo de $\mathcal{D}$ como en el caso anterior y agregando aristas aleatoriamente.
\end{itemize}

\begin{figure}
  \centering
  \subfloat[$\mathcal{D}$.]{\includegraphics[width=0.2\textwidth]{Chapter5/Figs/completeD.png}\label{fig:completeD}}
  \qquad
  \subfloat[$\mathcal{D}'$]{\includegraphics[width=0.2\textwidth]{Chapter5/Figs/incompleteD.png}\label{fig:incompleteD}}
%   \hfill
    \qquad
  \subfloat[$\mathcal{D}''$]{\includegraphics[width=0.2\textwidth]{Chapter5/Figs/wrongD.png}\label{fig:wrongD}}
  \caption{Ejemplo de los tres tipos de información con los que puede contar el algoritmo Q-learning. El grafo $\mathcal{D}$, es la estructura causal del modelo completo y correcto. $\mathcal{D'}$ y $\mathcal{D''}$ son el grafo con información parcial del modelo completo y la estructura causal sin todas las relaciones causales y además con relaciones ruidosas, respectivamente. El tipo de estructura
  del problema es uno-a-uno. Las aristas dirigidas punteadas describen conexiones espurias.}
  \label{fig:types-info-dag}
\end{figure}

Para medir el desempeño de los algoritmos en un experimento, se evalúa la recompensa
promedio sobre una serie de simulaciones.
Cada simulación consiste en ejecutar el algoritmo de aprendizaje durante $k$ episodios, en 
un ambiente con una estructura causal fija $\mathcal{D}$ y donde se busca alcanzar la meta $\mathbf{g}$.
La recompensa promedio para el $i$ ésimo episodio está dada por
$R^{i} = \frac{1}{H}\sum_{t=0}^H r(\mathbf{x}_t, \mathbf{g})$,
donde $H$ corresponde al tamaño del episodio.
El vector $\mathbf{R_i}$, del $i$ ésimo experimento contiene las recompensas promedio por cada episodio, y se define como
$\mathbf{R_i} = (R^{1}, \dots, R^k)$.

Finalmente, la medida de comparación entre algoritmos es
el promedio de los vectores $\mathbf{R_i}$, $i\in [1, M]$,  obtenidos en $M$ simulaciones. Esta medida, denotada como  $average$ puede escribirse como 
\begin{equation}
\label{eq:average}
average(\mathbf{R_1}, \dots, \mathbf{R_M}) = \frac{1}{M}(\sum^M_i \mathbf{R_{i}^1}, \dots, \sum^M_i\mathbf{R_{i}^k}),    
\end{equation}

donde $M$ es el número de simulaciones y el $\mathbf{R_i^j}$ indica la recompensa promedio obtenida en el $j$ ésimo episodio de la $i$ ésima simulación.

El parámetro $\epsilon$ se disminuye linealmente, donde
en cada selección de acción va decreciendo hasta llegar
un valor mínimo. La regla de actualización de $\epsilon$ en
el paso de tiempo $t$ se puede definir como $\epsilon = \max(\epsilon_{\min}, \epsilon_{\max} - \frac{|\epsilon_{\max} - \epsilon_{\min}|}{H \times k \times \delta} \times t)$, donde $H=N$ y $0 < \delta \leq 1$, es un factor para controlar
que tan rápido se alcanza el valor de $\epsilon$  mínimo, entre más cercano a 0,
termina más rápido la exploración.

De manera general se realizan tres experimentos. El primer experimento tiene 
como objetivo medir el desempeño al modificar la estructura causal $\mathcal{D}$ a diferentes porcentajes para obtener $\mathcal{D'}$ y $\mathcal{D}''$. El segundo experimento es con respecto a cambiar la tasa de decremento
de $\epsilon$ para llegar más rápido o lento a la fase de 
explotación constante. El tercer experimento, es probar
el algoritmo cuando no se tienen las variables
de alto nivel como observaciones directas, por lo tanto,
se trabaja sobre un espacio de estados continuo. En la tabla \ref{tab:exp-overview} se muestra una breve descripción de cada experimento.

Dado al tiempo que tarda cada simulación y los experimentos que se
propone realizar, se define a $M= 10$. Para las pruebas estadísticas, dado
este número de simulaciones, se necesita suponer un tamaño efecto de $2.0$ 
(un efecto grande según la escala
de \citet{cohen2013statistical}), un nivel de significancia de $0.05$, y
un poder estadístico de  $0.8$.
En general,
algunos de los parámetros que se comparten entre los experimentos se muestran en la tabla \ref{tab:switch-params}. 

Para comparar el método propuesto contra el algoritmo base, se sigue un esquema 
similar al presentado en la Sección \ref{sec:exp-taxi}. 
Se considera que un algoritmo ha resuelto la tarea si alcanza la 
un recompensa definida como ``óptima'' durante una racha de $E$ episodios.
Sin embargo, aquí no se cuenta
con un \textit{ground truth} o alguna referencia como en el caso del taxi.
Por simplicidad, para cada ejecución de las distintas configuraciones experimentales, 
se toma como óptima la recompensa promedio obtenida durante los últimos $S$ episodios del algoritmo Q-learning. Además, sólo se exige que el tamaño de las rachas sean $E=20$ para
los experimentos donde se varia el porcentaje de modificación del grafo y la 
explotación-exploración; y $E=10$ en el experimento con imágenes como observaciones del 
ambiente. Con esta información se realizan las pruebas estadísticas para
verificar si existe una diferencia entre los métodos con el grafo causal
($Q_2, Q_3$ y $Q_4$) con el algoritmo Q-learning sin datos extra ($Q_1$).

En cada experimento se muestran distintos medios que permiten 
la comparación del desempeño de los algoritmos: 

\begin{enumerate}
    \item Tablas con los resultados de las pruebas estadísticas para comparar el desempeño (con respecto al número episodios que tardan en alcanzar la recompensa definida como óptima en cada caso) de cada una de las variaciones del método 
    propuesto contra el método Q-learning sin modificaciones, en cada una de las diferentes configuraciones posibles del experimento.
\item Diagramas de caja y bigote (\textit{boxplots}) que resumen los
resultados de las tablas previas.
\item Una colección de curvas de aprendizaje con el desempeño de cada algoritmo.
\end{enumerate}

% En ésta se puede ver que para el caso de los ambientes estocásticos, la probabilidad de mover el interruptor que se eligió es $p_a = 0.75$ mientras que la probabilidad de accionar cualquier otro interruptor o ninguno, se distribuye uniformemente sobre el complemento. Además, se motiva al aprendizaje con un coeficiente $\alpha$ fijo con un valor alto.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\clearpage
\begin{table}[]
\centering
\caption{Resumen de los experimentos presentados en este capítulo.}
\label{tab:exp-overview}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Experimento & Descripción & Objetivo \\ \midrule
\begin{tabular}[c]{@{}l@{}}Variando el porcentaje de \\ modificación del grafo causal\end{tabular} & \begin{tabular}[c]{@{}l@{}}Este experimento consiste en modificar\\ el grafo $\mathcal{D}$ para obtener $\mathcal{D'}$ y $\mathcal{D''}$\\  a diferentes niveles. Cada uno de esos \\ niveles de modificación corresponden \\ al porcentajede conexiones que se \\ eliminan o que se añaden al grafo original.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si la información provista \\ por un modelo incompleto o \\ parcialmente  incorrecto ayuda y no \\ afecta negativamente el desempeño\\ del algoritmo de RL.\end{tabular} \\
Explotar o seguir explorando & \begin{tabular}[c]{@{}l@{}}Se controla qué tan rápido se desea alcanzar $\epsilon_{\min}$ \\ variando el parámetro $\delta$.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si reducir o aumentar las \\ consultas al grafo causal a lo largo del \\ aprendizaje afecta el desempeño\\ de los algoritmos.\end{tabular} \\
\begin{tabular}[c]{@{}l@{}}Utilizando observaciones \\ visuales del ambiente\end{tabular} & \begin{tabular}[c]{@{}l@{}}En este experimento no se tiene acceso a las \\ macro variables $\mathcal{X}$ directamente. \\ Sin embargo, el agente percibe imágenes del \\ estado del ambiente como observaciones.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si el modelo causal con\\ variables  en otro espacio siguen \\ conservando las propiedades de \\ ayudar en el aprendizaje como en los\\ casos discretos.\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[]
\centering
\caption{Valores para algunos parámetros que se comparten entre los distintos algoritmos. Se 
utiliza una tasa de aprendiza alta $\alpha$, dando más peso a la actualización. El factor de 
descuento $\gamma$ es cercano a $1$, dándole peso a la recompensa a largo plazo. El valor de $\epsilon$ se decrementa comenzando en $1$ y terminando en $0.1$. El número de acciones puede 
tomar los tres valores dados por $N$. El horizonte $H$ es igual al $N$ según sea el caso. El número de episodios es $200$ en los experimentos que usan DQN como algoritmo base. El número de experimentos es el mismo para todos los experimentos. Para las configuraciones estocásticas se utiliza una probabilidad de ejecutar las acción $p_a$, distribuyendo uniformemente la probabilidad de las otras acciones sobre $1-p_a$.}
\label{tab:switch-params}
\begin{tabular}{ll}
\hline
Parámetro                                                                                      & Valor    \\ \hline
$\alpha$                                                                                       & $0.8$      \\
$\gamma$                                                                                       & $0.95$     \\
$\epsilon_{\min}$                                                                              & $0.1$      \\
$\epsilon_{\max}$                                                                              & $1.0$      \\
$N$                                                                                            & \{5, 7, 9\} \\
$H$                                                                                            & \{5, 7, 9\} \\
$k$                                                                                            & \{200, 5000, 10000, 20000\}\\
$M$                                                                                            & 10       \\
$p_a$ & $0.75$      \\ \hline
\end{tabular}
\end{table}

\clearpage
\input{Chapter5/pmod_exp}
\clearpage
\input{Chapter5/delta_exp}
\clearpage
\input{Chapter5/dnq_exp}
\clearpage
\input{Chapter5/discussion}