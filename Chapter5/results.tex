\chapter{Experimentos y resultados}\label{chapter5}

% **************************** Define Graphics Path **************************
\graphicspath{{Chapter5/Figs/}}


En este capítulo se describe la experimentación y los resultados obtenidos
que muestran la mejora en el desempeño de un agente que aprende
con y sin información adicional de un grafo causal.
Para probar el concepto propuesto se atacan dos problemas: la tarea de clásica del taxi \cite{Dietterich:2000:HRL:1622262.1622268} y la de los
interruptores de luz, descrita en el capítulo \ref{chapter4}.
En resumen, los experimentos consisten en integrar el grafo $\mathcal{D}$ a la política $\epsilon$-greedy
en el algoritmo $Q$-learning \cite{watkins1992q}.
En la política $\epsilon$-greedy en vez de mantener fijo a $\epsilon$, se propone empezar motivando al agente a explorar y usar
el modelo causal e ir disminuyendo $\epsilon$ para dar más peso a la explotación.
Se comparan cuatro algoritmos, \textit{Q-learning sin información
adicional}, \textit{Q-learning con una estructural causal completa}, \textit{Q-learning con una estructura parcial} y un \textit{Q-learning con una estructura incorrecta}.
Dependiendo de la configuración experimental, cada uno de los algoritmos se ejecuta en una versión determinista y otra estocástica del ambiente. 
En las siguientes secciones se describen a detalle los experimentos realizados y los resultados. Todo el software desarrollado está 
disponible en \url{https://github.com/ivanfeliciano/causal_rl/}.


\section{Problema del taxi}

En esta sección se muestran los experimentos y resultados del método propuesto para el problema clásico del taxi \cite{Dietterich:2000:HRL:1622262.1622268}.
Se realiza un par de experimentos sobre dos configuraciones del ambiente diferentes,
una determinista y otra estocástica. De acuerdo con los resultados, 
el algoritmo usando un grafo causal como ``oráculo'' para seleccionar sus acciones
tiene un mejor desempeño en el ambiente determinista pero no hay una diferencia estadísticamente significativa en el ambiente estocástico.

\subsection{Descripción de la tarea}

El primer problema a resolver es la tarea clásica del taxi \cite{Dietterich:2000:HRL:1622262.1622268}.
La Figura \ref{fig:taxi} muestra gráficamente el problema.
Existen cuatro posiciones en el mundo marcadas como R, B, G, y Y. 
La tarea es episódica y en cada episodio, 
el taxi comienza en un cuadro aleatoriamente elegido. 
Existe un pasajero en una de la cuatro posiciones (también elegida
aleatoriamente), y el pasajero desea ser transportado a una de las
cuatro zonas.
El taxi debe dirigirse a la posición del pasajero, recogerlo, ir a su destino y dejarlo.
El episodio termina cuando el pasajero es dejado en su destino.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{Chapter5/Figs/taxi-env.pdf}
    \caption{La cuadrícula del ambiente del taxi. El taxi se encuentra en el cuadro central. En esta configuración del mundo, el objetivo es recoger al pasajero en la posición R y llevarlo a la posición G.}
    \label{fig:taxi}
\end{figure}

El conjunto de acciones $\mathcal{A}$ está compuesto por seis elementos: una acción para recoger $a_1$, una para dejar $a_2$ y
cuatro acciones de 
navegación que trasladan al taxi un cuadro al norte, sur, 
este u oeste, denotadas por $a_3, \dots, a_6$, respectivamente.
Existe una recompensa de -1 por cada acción, una recompensa adicional de 20 por cada pasajero llevado a su destino 
exitosamente y una penalización de -10 por acciones ilegales
de recolección y dejado.
El espacio de estados $\mathcal{S}$ tiene como elementos 
500 tuplas de tres elementos donde describen los 25 cuadros, las 5 posiciones del pasajero (incluyendo cuando está en el taxi) y los 4 destinos.


% Aquí ahondar más sobre el conjunto X y G, quienes lo compoenen y como está de pequeñito el problema
El conjunto $\mathcal{X}$ contiene 4 variables que traducen las tuplas con las posiciones del taxi y del pasajero en variables binarias. $x_1$
es la variable que dice si el taxi está en la misma posición que el
pasajero, $x_2$ es la variable que denota si el pasajero es llevado dentro del taxi, $x_3$ describe si el taxi está en la posición destino,
y $x_4$ es la variable que representa al estado de que el pasajero es entregado correctamente. Es una tarea relativamente simple y $x_1 \neq x_3$, por lo tanto las metas son $\mathcal{G} = \{\mathbf{g_1}, \mathbf{g_2}\}$, donde $\mathbf{g_1} = [1, 1, 0 , 0]$ y $\mathbf{g_2} = [0,1,1,1]$. El primer vector se puede ver como el sub objetivo de que el pasajero aborda el taxi y el segundo vector es la meta general, 
entregar al pasajero en su destino.
El grafo causal $\mathcal{D}$ entre las variables de acción y los estados se puede 
ver en la Figura \ref{fig:cm-taxi}. Para este problema, los efectos
necesitan de todas sus causas para suceder, por ejemplo, para que el
pasajero esté dentro del taxi, $x_2 = 1$, entonces se debe actuar
subiéndolo al vehículo $a_1$ y además estar en la misma ubicación que
el pasajero.

\begin{figure}
    \centering
    \includegraphics[scale=0.3]{Chapter5/Figs/causal_structure_taxi.png}
    \caption{Propuesta de una estructura causal para la tarea del taxi. Los nodos azules
    denotan a las acciones recoger y dejar, $a_1$ y $a_2$ respectivamente. Los nodos verdes son las variables que describen algunos estados del ambiente; $x_1$
es la variable que indica si el taxi está en la misma posición que el
pasajero, $x_2$  si el pasajero es llevado dentro del taxi, $x_3$ corresponde a si el taxi está en la posición destino y $x_4$ que indica que el pasajero es entregado correctamente.}
    \label{fig:cm-taxi}
\end{figure}


\subsection{Configuración experimental}

Debido a la poca información que ofrece el grafo $\mathcal{D}$ y al tamaño de $\mathcal{G}$, para este problema no se ahonda en realizar experimentos con diferentes configuraciones. Por lo
tanto, sólo se comparan dos algoritmos,
el algoritmo Q-learning con y sin la estructura causal.
Además, los dos algoritmos se prueban sobre dos versiones del ambiente, una determinista
y otra estocástica. Los valores de los parámetros para los experimentos se muestran en la tabla \ref{tab:tax-params}. Se fija una tasa de aprendizaje $\alpha$ para dar más peso al TD-error. Se ejecutan $M$ experimentos y cada experimento consiste de ejecutar cada algoritmo $k$ de episodios.  El valor de $\epsilon$  se va actualizando con respecto a la regla $\epsilon = \max(\epsilon_{\min}, \epsilon_{\max} - \frac{|\epsilon_{\max} - \epsilon_{\min}|}{H \times k \times \delta} \times t)$. Donde, de acuerdo con las especificaciones del ambiente de \cite{gym2016brockman} $H = 200$. Además, se opta por que se alcance el mínimo valor de $\epsilon_{\min}$ después del primer cuarto del ciclo de entrenamiento, haciendo $\delta = 0.25$. Para el caso donde se hace frente a un ambiente estocástico, la probabilidad de ejecutar la acción que se seleccionó es $p_a = 0.7$ mientras que la probabilidad de realizar cualquiera de las otras acciones se distribuye uniformemente sobre el complemento. Finalmente, como medida
de desempeño se usa la recompensa promedio por episodio.

\begin{table}[h]
\centering
\caption{Parámetros para los algoritmo Q-learning y Q-learning con la estructura causal.}
\label{tab:tax-params}
\begin{tabular}{ll}
\hline
Parámetro                                                                                      & Valor    \\ \hline
$\alpha$                                                                                       & $0.8$      \\
$\gamma$                                                                                       & $0.95$     \\
$\epsilon_{\min}$                                                                              & $0.1$     \\
$\epsilon_{\max}$                                                                              & $1.0$      \\
$k$                                                                                            & 1000     \\
$M$                                                                                            & 25       \\
$H$ & 200\\
$\delta$ & $0.25$\\
$p_a$ & $0.7$      \\ \hline
\end{tabular}
\end{table}

\subsection{Resultados}

En la Figura \ref{fig:results-taxi} se muestran los resultados obtenidos
para ambas configuraciones del ambiente (determinista y estocástico) donde
los valores de los parámetros se describen en la tabla \ref{tab:tax-params}.
La recompensa promedio por episodio para el método propuesto y para el algoritmo
original Q-learning están en color naranja y azul, respectivamente. 
Los resultados muestran que el algoritmo Q-learning guiado por el grafo da un salto inicial alto y mantiene una recompensa promedio mucho mayor hasta que ambos métodos convergen. Esto es esperado, ya que
no inicia una exploración a ciegas. En ambas versiones del ambiente, los algoritmos parecen comportarse de manera similar a partir del episodio 400.

\begin{figure}[H]
  \centering
  \subfloat[Ambiente determinista.]{\includegraphics[width=0.45\textwidth]{Chapter5/Figs/taxi_env_comparison_det_1000_25_eps_30000.pdf}\label{fig:taxi-rew-det}}
  \hfill
  \subfloat[Ambiente estocástico.]{\includegraphics[width=0.45\textwidth]{Chapter5/Figs/taxi_env_comparison_sto_1000_25_eps_30000.pdf}\label{fig:taxi-rew-sto}}
  \caption{Comparación del desempeño para los dos algoritmos en 1000 episodios. La región sombreada es la desviación estándar para 50 experimentos.}
  \label{fig:results-taxi}
\end{figure}


La tarea para el ambiente determinista se considera resuelta cuando se obtiene una recompensa promedio de 9.7 durante 100 episodios consecutivos. Sin embargo, para fines de este experimento se propone relajar este parámetro. Se calculó la recompensa promedio en los últimos 100 experimentos del algoritmo Q-learning sin información extra, la cual es 0 y -30 para el ambiente determinista y estocástico, respectivamente. Por lo tanto, para considerar la tareas resuelta y con eso medir qué algoritmo es mejor, se utilizan como valores de recompensa óptima al 0 y -30, según corresponda. En la tabla \ref{tab:resultados-taxi} se muestran los resultados del número de episodios, promedio sobre $M$ experimentos,
que tarda cada algoritmo en alcanzar la recompensa óptima propuesta durante una racha de más de 100 episodios. Entre menor sea el número de episodios en alcanzar la racha denota que un algoritmo aprendió más rápido.

De acuerdo con los resultados de la tabla \ref{tab:resultados-taxi}, el algoritmo que alcanza la recompensa óptima antes, es el método Q-learning con la información causal. Sin embargo, con el fin de validar los resultados se utiliza la prueba de Welch como se sugiere en \cite{colas2019hitchhikers}. La hipótesis nula $h_0$ corresponde a que el número de episodios en alcanzar la recompensa óptima durante más de 100 episodios seguidos es igual para ambos algoritmos. Para el caso del ambiente determinista la hipótesis es rechazada con $p < 0.05$. Sin embargo, para el caso del ambiente estocástico se falla al rechazar $h_0$ ya que $p > 0.05$. Por lo tanto, a pesar de que los resultados en el Cuando 
\ref{tab:resultados-taxi} indican un mejor comportamiento para el algoritmo con la estructura causal auxiliando la selección de acciones, no hay diferencia estadísticamente significativa en la versión estocástica del ambiente.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!h]
\centering
\caption{Resultados del número de episodios promedio y la desviación estándar en alcanzar una recompensa > 0 con una racha > 100 y > -30 con una racha > 100 para los ambientes determinista y estocástico, respectivamente.}
\label{tab:resultados-taxi}
\begin{tabular}{@{}lll@{}}
\toprule
Algoritmo & \begin{tabular}[c]{@{}l@{}}Ambiente determinista.\\ Número de episodios\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ambiente estocástico.\\ Número de episodios\end{tabular} \\ \midrule
Q-learning & $632 \pm 69$ & $496 \pm 247$ \\
\begin{tabular}[c]{@{}l@{}}Q-learning +\\ estructura causal\end{tabular} & $\mathbf{589 \pm 106}$ & $476 \pm 232$ \\ \bottomrule
\end{tabular}
\end{table}

Los resultados obtenidos son intuitivos, esto es, comenzar una exploración a ciegas es 
peor que conocer un poco del mundo por eso el valor de recompensa inicial mayor del algoritmo propuesto. 
% Por otro lado, ambos algoritmos se comportan de manera similar después de unos cientos de episodios.
Por otro lado, la información del mundo con el que cuenta el agente es muy limitada. En realidad solo se cuenta con la información de lo que provocan dos de las seis acciones posibles. Esto puede ser la razón por la que la diferencia entre los algoritmos 
en un ambiente estocástico no es significativa. Tener un modelo pequeño sumando un ambiente donde no siempre intentar una acción te lleva a un estado deseado conduce a un paso de exploración más. Esto por lo tanto, provoca que ambos algoritmos sean muy parecidos.
 
\section{Problema de los interruptores de luz}

En esta sección se muestran los experimentos y resultados del método propuesto para
el problema de los interruptores de luz descrito en el capítulo \ref{chapter4}.
A diferencia del ambiente del taxi, este problema cuenta 
con diferentes variaciones. Por ejemplo,
diferentes tipos de conexiones en la estructura subyacente del problema, el tamaño de dicha estructura, etc.
En general, de acuerdo con los experimentos, el algoritmo usando un grafo causal como ``oráculo'' para seleccionar sus acciones tiene un mejor desempeño incluso en 
configuraciones donde las estructuras causales están parcialmente incorrectas o también 
al disminuir la tasa de consultas al modelo en una parte temprana del aprendizaje.


\subsection{Descripción de la tarea}
Para los experimentos de esta sección se ataca la tarea de control de interruptores de luz propuesta en \cite{nair2019causal} y descrita de manera general en la Sección \ref{section:switches-example}. Un agente tiene el control
de $N$ interruptores que controlan $N$ luces en un sitio.
Cada acción $a\in \mathcal{A}$ corresponde a mover un interruptor o 
a no mover ninguno, por lo tanto $|\mathcal{A}| = N + 1$.
El agente puede percibir dos tipos de señales del ambiente,
una imagen $s$ con una vista cenital del sitio, o vectores binarios $x \in \{0,1\}^N$ de 
macro-variables que codifican las luces prendidas, donde
$x_i = 1$ si la luz en la zona $i$ está prendida, de otro modo 
toma el valor $x_i = 0$.

Se exploran tres tipos de estructuras causales entre los
interruptores y las luces: \textit{uno-a-uno},
\textit{causa común} y \textit{efecto común}.
En los problemas con estructuras uno-a-uno cada interruptor corresponde a una sola luz.
Para el segundo tipo, de causa común, todas
las luces son controladas a lo más por un interruptor pero un
solo interruptor puede controlar más de una luz.
El tercer caso son estructuras de efecto común, donde cada interruptor
controla una sola luz, aunque múltiples interruptores
pueden controlar la misma luz. De manera visual, los tres tipos de estructuras
se muestran en la Figura \ref{fig:struct}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{Chapter5/Figs/switches_struct.png}
    \caption{Tipos de estructuras causales subyacentes posibles.}
    \label{fig:struct}
\end{figure}
La recompensa inmediata $r$ brindada al agente se calcula obteniendo la distancia entre el vector de variables de estado alto nivel $\mathbf{x}$ y el vector meta $\mathbf{g}$. En este problema se usa la distancia euclidiana.


\subsection{Configuración experimental general}

En las siguientes secciones se compara el desempeño 
del método propuesto en diferentes escenarios, principalmente, para mostrar 
las posibilidades y ventajas de usar un información del grafo causal, completa, incompleta e incluso incorrecta. 
% A pesar de ser diferentes
% experimentos, éstos comparten la configuración de algunos elementos, por ejemplo, 
% la medida de desempeño, el número de variables $N$, etc.
Se comparan cuatro algoritmos, teniendo como base
el método Q-learning. Donde la diferencia subyace en la cantidad y calidad de la información adicional con la que cuenta. A continuación, se describen de manera breve los métodos
comparados.

\begin{itemize}
    \item \textit{Q-learning sin información adicional} ($Q_1$). Este método sirve como
    línea de base para medir que tanto mejora el aprendizaje. El algoritmo, dependiendo del espacio de estados sobre el que se trabaje, es el 
    método básico de Q-learning \cite{watkins1992q} o Q-learning profundo \cite{mnih2013playing}, para estados
    discretos y continuos, respectivamente. La selección de acciones se lleva a cabo mediante una política $\epsilon$ greedy clásica.
    \item \textit{Q-learning + estructura causal completa}($Q_2$). Durante la política de selección de acciones, el agente cuenta con la estructura causal del ambiente completa y verdadera $\mathcal{D}$.
    \item \textit{Q-learning + estructura causal incompleta}($Q_3$). En este caso, el agente cuenta con un subgrafo $\mathcal{D'}$ del grafo $\mathcal{D}$. Este subgrafo se genera eliminando aristas de $\mathcal{D}$ aleatoriamente.
    \item \textit{Q-learning + estructura causal incorrecta}($Q_4$). Este algoritmo consulta un modelo $\mathcal{D}''$ con relaciones espurias y sin algunas relaciones verdaderas. Este grafo $\mathcal{D}''$ se obtiene generando un subgrafo de $\mathcal{D}$ como en el caso anterior y agregando aristas aleatoriamente.
\end{itemize}

\begin{figure}
  \centering
  \subfloat[$\mathcal{D}$.]{\includegraphics[width=0.15\textwidth]{Chapter5/Figs/completeD.png}\label{fig:completeD}}
  \qquad
  \subfloat[$\mathcal{D}'$]{\includegraphics[width=0.15\textwidth]{Chapter5/Figs/incompleteD.png}\label{fig:incompleteD}}
%   \hfill
    \qquad
  \subfloat[$\mathcal{D}''$]{\includegraphics[width=0.15\textwidth]{Chapter5/Figs/wrongD.png}\label{fig:wrongD}}
  \caption{Ejemplo de los tres tipos de información con los que puede  contar el algoritmo Q-learning. El tipo de estructura
  del problema es uno-a-uno. Las aristas dirigidas punteadas describen conexiones espurias.}
  \label{fig:types-info-dag}
\end{figure}

Para medir el desempeño de los algoritmos se evalúa la recompensa
promedio sobre una serie de experimentos.
Cada experimento consiste en ejecutar el algoritmo de aprendizaje durante $k$ episodios, en 
un ambiente con una estructura causal fija $\mathcal{D}$ y donde se busca alcanzar la meta $\mathbf{g}$.
La recompensa promedio para el $i$ ésimo episodio está dada por
$R^{i} = \frac{1}{H}\sum_{t=0}^H r(\mathbf{x}_t, \mathbf{g})$,
donde $H$ corresponde al tamaño del episodio.
El vector $\mathbf{R_i}$, del $i$ ésimo experimento contiene las recompensas promedio por cada episodio, y se define como
$\mathbf{R_i} = (R^{1}, \dots, R^k)$.

Finalmente, la medida de comparación entre algoritmos es
el promedio de los vectores $\mathbf{R_i}$, $i\in [1, M]$,  obtenidos en $M$ experimentos. Esta medida, denotada como  $average$ puede escribirse como 
\begin{equation}
\label{eq:average}
average(\mathbf{R_1}, \dots, \mathbf{R_M}) = \frac{1}{M}(\sum^M_i \mathbf{R_{i}^1}, \dots, \sum^M_i\mathbf{R_{i}^k}),    
\end{equation}

donde $M$ es el número de experimentos y el $\mathbf{R_i^j}$ indica la recompensa promedio obtenida en el $j$ ésimo episodio del $i$ ésimo experimento.

El parámetro $\epsilon$ se disminuye linealmente, donde
en cada selección de acción va decreciendo hasta llegar
un valor mínimo. La regla de actualización de $\epsilon$ en
el paso de tiempo $t$ se puede definir como $\epsilon = \max(\epsilon_{\min}, \epsilon_{\max} - \frac{|\epsilon_{\max} - \epsilon_{\min}|}{H \times k \times \delta} \times t)$, donde $H=N$ y $0 < \delta \leq 1$, es un factor para controlar
que tan rápido se alcanza el valor de $\epsilon$  mínimo, entre más cercano a 0,
termina más rápido la exploración.

De manera general se realizan tres tipos de experimentos. El primer experimento tiene 
como objetivo medir el desempeño al modificar la estructura causal $\mathcal{D}$ a diferentes porcentajes para obtener $\mathcal{D'}$ y $\mathcal{D}''$. El segundo experimento es con respecto a cambiar la tasa de decremento
de $\epsilon$ para llegar más rápido o lento a explotar 
más constantemente. El tercer experimento, es probar
el algoritmo cuando no se tienen las variables
de alto nivel como observaciones directas, por lo tanto,
se trabaja sobre un espacio de estados continuo. En la tabla \ref{tab:exp-overview} se muestra una breve descripción de cada experimento.

En general,
algunos de los parámetros que se comparten entre los experimentos se muestran en la tabla \ref{tab:switch-params}. 
% En ésta se puede ver que para el caso de los ambientes estocásticos, la probabilidad de mover el interruptor que se eligió es $p_a = 0.75$ mientras que la probabilidad de accionar cualquier otro interruptor o ninguno, se distribuye uniformemente sobre el complemento. Además, se motiva al aprendizaje con un coeficiente $\alpha$ fijo con un valor alto.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{Resumen de los experimentos presentados en este capítulo.}
\label{tab:exp-overview}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Experimento & Descripción & Objetivo \\ \midrule
\begin{tabular}[c]{@{}l@{}}Variando el porcentaje de \\ modificación del grafo causal\end{tabular} & \begin{tabular}[c]{@{}l@{}}Este experimento consiste en modificar\\ el grafo $\mathcal{D}$ para obtener $\mathcal{D'}$ y $\mathcal{D''}$\\  a diferentes niveles. Cada uno de esos \\ niveles de modificación corresponden \\ al porcentajede conexiones que se \\ eliminan o que se añaden al grafo original.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si la información provista \\ por un modelo incompleto o \\ parcialmente  incorrecto ayuda y no \\ afecta negativamente el desempeño\\ del algoritmo de RL.\end{tabular} \\
Explotar o seguir explorando & \begin{tabular}[c]{@{}l@{}}Se controla qué tan rápido se desea alcanzar $\epsilon_{\min}$ \\ variando el parámetro $\delta$.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si reducir o aumentar las \\ consultas al grafo causal a lo largo del \\ aprendizaje afecta el desempeño\\ de los algoritmos.\end{tabular} \\
\begin{tabular}[c]{@{}l@{}}Utilizando observaciones \\ visuales del ambiente\end{tabular} & \begin{tabular}[c]{@{}l@{}}En este experimento no se tiene acceso a las \\ macro variables $\mathcal{X}$ directamente. \\ Sin embargo, el agente percibe imágenes del \\ estado del ambiente como observaciones.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Determinar si el modelo causal con\\ variables  en otro espacio siguen \\ conservando las propiedades de \\ ayudar en el aprendizaje como en los\\ casos discretos.\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[]
\centering
\caption{Valores para algunos parámetros que se comparten entre los distintos algoritmos. Se 
utiliza una tasa de aprendiza alta $\alpha$, dando más peso a la actualización. El factor de 
descuento $\gamma$ es cercano a $1$, dándole peso a la recompensa a largo plazo. El valor de $\epsilon$ se decrementa comenzando en $1$ y terminando en $0.1$. El número de acciones puede 
tomar los tres valores dados por $N$. El horizonte $H$ es igual al $N$ según sea el caso. El número de episodios es $200$ en los experimentos que usan DQN como algoritmo base. El número de experimentos es el mismo para todos los experimentos. Para las configuraciones estocásticas se utiliza una probabilidad de ejecutar las acción $p_a$, distribuyendo uniformemente la probabilidad de las otras acciones sobre $1-p_a$.}
\label{tab:switch-params}
\begin{tabular}{ll}
\hline
Parámetro                                                                                      & Valor    \\ \hline
$\alpha$                                                                                       & $0.8$      \\
$\gamma$                                                                                       & $0.95$     \\
$\epsilon_{\min}$                                                                              & $0.1$      \\
$\epsilon_{\max}$                                                                              & $1.0$      \\
$N$                                                                                            & \{5, 7, 9\} \\
$H$                                                                                            & \{5, 7, 9\} \\
$k$                                                                                            & \{200, 5000, 10000, 20000\}\\
$M$                                                                                            & 10       \\
$p_a$ & $0.75$      \\ \hline
\end{tabular}
\end{table}

\clearpage
\input{Chapter5/pmod_exp}
\clearpage
\input{Chapter5/delta_exp}
\clearpage
\input{Chapter5/dnq_exp}
\clearpage
\input{Chapter5/discussion}