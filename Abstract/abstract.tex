% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
Reinforcement learning is an indispensable learning by interaction paradigm within machine learning. One of the challenges that emerge in Reinforcement Learning, is the trade-off between exploration and exploitation. To solve this problem, an agent can limit her search space by leveraging the properties of her environment or using previous knowledge. Specifically, the agent can exploit the causal relationships of her world. We propose to attend the learning process of an agent in goal-directed tasks where a causal structure is provided. The causal model helps to restrict the search space by reducing the actions that an agent can take through graph queries like check what variables are direct causes of variables of interest. This simulates common sense that lightens the time it takes the trial and error approach. Our main contribution is the framework to represent the causal information and an algorithm to guide action selection by querying the causal graph. We cope with a couple of small and simple problems on a discrete and continuous domains. We also show that using extra information from a causal structure in the Q-learning action selection step leads to a higher and faster jump-start reward and stability, respectively. Furthermore, it is shown that it is not mandatory to have a completely correct causal structure, since
a better performance is obtained, even using partial and some spurious relationships in the graphs than not using extra information.

\end{abstract}




