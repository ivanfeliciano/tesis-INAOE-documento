
\begin{resumen}


El aprendizaje por refuerzo es un paradigma de aprendizaje por interacción 
% indispensable 
dentro del aprendizaje automático. Uno de los principales retos que surgen en 
el aprendizaje por refuerzo, es el compromiso entre la 
explotación de la información que ya se conoce o 
exploración del ambiente. Para ayudar a resolver este problema, 
un agente puede limitar su espacio de búsqueda al 
aprovechar propiedades de su ambiente o utilizar 
conocimiento dado previamente. Específicamente, el agente
puede explotar las relaciones causales de su mundo. En este
trabajo se propone auxiliar el proceso de aprendizaje de
un agente en tareas dirigidas por metas donde la estructura
causal es brindada. 
Un modelo causal ayuda a restringir el espacio de búsqueda, al
guiar la toma de decisiones de un agente, mediante consultas en un grafo causal. Por ejemplo, verificando qué variables 
son causas directas de otras.
Este conocimiento causal permite disminuir el
tiempo que toma el enfoque de prueba y error.
Entre las contribuciones de esta investigación están: 1)
la representación de la información extra en un grafo causal y 2) un algoritmo para guiar la selección de acciones 
a través de consultas en el grafo.
Se ataca un par de problemas sobre dominios discretos y continuos que son relativamente pequeños y simples para
probar el concepto propuesto.
Se muestra, a través de diferente experimentos, que
usar información adicional de una estructura causal
en la selección de acciones del algoritmo Q-learning
lleva a una recompensa final y a un salto inicial de
recompensa mayores. Además, se muestra que
no es obligatorio contar con una estructura causal 
completa y correcta. Incluso, guiar a una agente con una estructura parcial o con relaciones falsas se alcanza un mejor desempeño que no utilizar información suplementaria.

\end{resumen}