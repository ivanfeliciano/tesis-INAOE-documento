\chapter{Introducción}\label{chapter1}



\graphicspath{{Chapter1/Figs/}}

Uno de los objetivos de la inteligencia artificial (IA), es crear agentes autónomos que aprendan
a través de la interacción con su ambiente \cite{surveyDRL2017}.
% La noción de aprender mediante interacciones es común para el ser humano.
% Éste extrae reglas y experiencia de la interacción con la naturaleza
% y utiliza esta información para cumplir una meta y mejorar su comportamiento con el paso del tiempo.
Un marco de trabajo para el aprendizaje por interacción es el \textit{aprendizaje por
refuerzo}.


El aprendizaje por refuerzo (RL por sus siglas en inglés) estudia cómo un agente puede aprender, a través de la percepción del estado de su ambiente,
a elegir acciones que maximicen recompensas futuras. 
Normalmente, no se le dice al agente qué acciones
tomar, en cambio, éste debe descubrir qué acciones, al intentarlas, producen la mayor recompensa.
En los casos más interesantes, las acciones no sólo afectan recompensas inmediatas
sino también el siguiente estado y, a través de eso, todas las recompensas ulteriores \cite{sutton_barto_2018}.

Los algoritmos de aprendizaje por refuerzo 
han mostrado ser efectivos en diversos dominios como en videojuegos \cite{mnih2013playing, vinyals2019grandmaster}, 
robótica \cite{openai2019solving}
y atención médica \cite{gottesman2018evaluating}.
Sin embargo, la mayoría de los  sistemas de aprendizaje por refuerzo actuales sufren de algunas deficiencias. Una de ellas es que no aprovechan procesos de
alto nivel para 
explotar patrones más allá de los asociativos \cite{garnelo2016deep, playingagainstnature2018}. Entre estos procesos de alto nivel se encuentra el \textit{razonamiento causal} \cite{pearl_2009, spirtes2000causation}. Las técnicas más comunes
en el aprendizaje por refuerzo ignoran \textit{relaciones causales} entre eventos.

Se tiene como hipótesis que conocer relaciones causales entre eventos podría permitir a un agente 
utilizar este conocimiento extra
para predecir qué causará realizar cierta acción
y entonces acelerar su proceso de aprendizaje.
Además, la capacidad de razonar sobre causas y efectos es parte integral de
la inteligencia natural \cite{nair2019causal}.

Conocer relaciones causales entre eventos podría permitir a un agente 
utilizar este conocimiento extra
para predecir qué causará realizar cierta acción
y entonces acelerar su proceso de aprendizaje.
Además, la capacidad de razonar sobre causas y efectos es parte integral de
la inteligencia natural \cite{nair2019causal}.

Como se menciona en \cite{theoryofcausalities2006} 
la causalidad no es un concepto monolítico y tampoco
existe una única entidad que sustente el uso correcto
del concepto. Existen muchas formas de definirla según diferentes contextos. Las teorías de causalidad
parecen ser un buen paradigma para algunas aplicaciones,
sin embargo, cada una tiene contraejemplos y problemas.
Entre algunos de los diferentes enfoques 
están; la causalidad para
el orden de eventos propuesta por Lamport \cite{lamport2019time} y otras teorías revisadas en
\cite{holland1986statistics} como la causalidad de Wiener-Granger para
series de tiempo \cite{granger1969investigating} y la 
causalidad probabilista de Suppes \cite{suppes1973probabilistic}. 



% \textit{En este trabajo, la teoría
% de causalidad que se sigue es la del enfoque \textit{intervencionista} propuesta
% por Judea Pearl \cite{pearl2010introduction} \cite{sep-causal-models}.
% En \cite{pearl2018bookofwhy} se describe un
% nuevo paradigma de aprendizaje cuyo objetivo es descubrir relaciones de causa y efecto entre diferentes variables llamado \textit{inferencia causal} (CI por su siglas en inglés).
% En esta área, la causalidad se define
% como que $X$ causa $Y$ (denotado como $X \rightarrow Y$) si
% y sólo si una intervención 
% o manipulación de $X$ tiene un 
% efecto en $Y$\cite{doi:10.1063/1.5025050}.
% Una intervención va más allá que las observaciones pasivas pues asegura que el cambio
% observado en $Y$ sea debido a $X$ y que no
% es por una posible confusión con otros factores.
% }


Este trabajo sigue el enfoque de la teoría de causalidad \textit{manipulacionista} \cite{sep-causation-mani}. En ésta, la idea
fundamental sobre causalidad se puede describir de manera intuitiva como: si $A$ es genuinamente una causa de $X$, entonces si se manipula $A$ de la manera correcta, esto debe ser una forma de manipular o cambiar $X$ \cite{campbell1979quasi, woodward2005making}. En específico, 
se propone utilizar el marco de trabajo manipulacionista, a través de \textit{intervenciones}, que
ha sido desarrollado ampliamente por Judea Pearl \cite{pearl_2009}. En su trabajo Pearl ha 
utilizado sistemas de ecuaciones y grafos dirigidos para representar relaciones causales. Además en
\cite{pearl2018bookofwhy}, Pearl describe un nuevo paradigma de aprendizaje cuyo objetivo es descubrir relaciones de causa y efecto entre diferentes variables llamado \textit{inferencia causal} (CI por su siglas en inglés).





Las relaciones causales dentro de un sistema se pueden representar mediante un modelo matemático, conocido como modelo causal \cite{sep-causal-models}.
El modelo causal de un sistema
permite predecir qué pasaría si algunas
variables son intervenidas, predecir las salidas de casos que nunca han sido observados \cite{chaochao_2019} y responder preguntas como:
\textit{¿Si deseo esta salida, qué acción necesito
llevar a cabo?}


% Por qué es importante para una IA general


% Decir qué limitaciones tenemos

% y finalmente lo que proponemos, lo que buscamos



% % Se tiene como hipótesis que conocer tales relaciones podría permitir a un agente 
% % utilizar este conocimiento extra
% % para predecir qué causará realizar cierta acción
% % y entonces acelerar su proceso de aprendizaje.
% % Además, la capacidad de razonar sobre causas y efectos es parte integral de
% % la inteligencia natural \cite{nair2019causal}.



% \begin{itemize}
%     \item Poner el trabajo en una perspectiva mayor.
%     \item Visión de conjunto mostrando cómo el área del tema se relaciona con el resto del 
%     campo de las ciencias computacionales
%     \item Hacer un resumen de las ideas clave del trabajo, en particular resaltando 
%     los beneficios de mi trabajo.
%     \item La introducción contiene los siguientes elementos:
%     \begin{itemize}
%         \item \textbf{Tema:} indicando el \textit{tema en específico} del reporte; esto
%         se debe hacer tan pronto como sea posible, preferentemente en el primer
%         párrafo del reporte.
%         \item \textbf{Propósito y situación:} indicando por qué el reporte fue escrito
%         y cuál fue el propósito.
%         \item \textbf{Antecedentes del tema:} provee definiciones clave
%         y preliminares básicos importantes para el lector; debe 
%         capturar el interés del lector.
%         \item \textbf{Visión general del reporte:} describe el 
%         esquema general del reporte. Esto informa al lector
%         que esperar, y hace más fácil entender el material y hacer 
%         transiciones entre secciones.
        
%     \end{itemize}
    
% \end{itemize}






Este razonamiento causal puede hacer 
que los agentes de RL, o sistemas de inteligencia artificial en general, 
sean más eficientes.
Por ejemplo, un robot que entiende que dejar caer cosas hace que se rompan, no necesitaría tirar cientos de objetos frágiles al suelo para ver qué les sucede.




% ¿Cuáles son los objetivos y las principales metas de investigación y porqué son importantes?

La finalidad de este trabajo de investigación es 
acelerar el entrenamiento de un agente de RL
a través de la selección de acciones guiadas por un 
modelo causal para completar tareas que tienen una
estructura causal subyacente.

En general, un agente comienza su búsqueda a ciegas,
mediante interacciones a prueba y error.
Sin embargo, éste puede, a través de intervenciones
en un modelo causal, hacer consultas del tipo: \textit{¿Qué tal si hago ...?} Estas intervenciones sobre variables 
que describen a las acciones permiten usar al modelo
como ``oráculo'' para no realizar acciones que lleven
a estados no deseados o para no evitar la acción que
lleve a una meta.

Se ha probado que la formulación actual del aprendizaje
por refuerzo no es un problema causal \cite{gonzalezsoto2019reinforcement}.
Por lo tanto, es una suposición fuerte 
decir que todos los problemas 
en aprendizaje por refuerzo contienen mecanismos causales. 
Sin embargo, hay tareas en las
que un experto o incluso el mismo algoritmo puede
aprender el modelo causal latente.
Problemas con una estructura causal latente
se pueden encontrar en robótica, ambientes como Animal AI \cite{beyret2019animalai}, 
tareas dirigidas por metas \cite{nair2019causal} e
incluso algunos juegos \cite{madumal2019explainable}.


% ¿Qué aspectos específicos del problema general se atacarán?
% % ¿Cuál es la razón o justificación para seguir esta línea de investigación?
% ¿Se tienen conclusiones o resultados 
% preliminares?, ¿cuáles y de qué tipo son?

Los autores en \cite{lattimore2016causal} han mostrado
que es posible mejorar la identificación de acciones que proporcionan una recompensa mayor en
el problema del bandido dado el grafo causal. Además, en \cite{nair2019causal}
se muestra que una estructura causal permite que una política condicionada a una
meta (aprendida mediante aprendizaje por imitación) generalice para instancias no vistas de 
un problema. 
La principal diferencia con estos trabajos es que se atacan tareas con descripciones discretas y continuas de su ambiente; y se asiste el aprendizaje de una política, a través de la función de valor, utilizando un paradigma por refuerzo clásico.
El modelo causal se supone conocido, y éste permite una selección de acciones 
guiada para la inmediata actualización de la política o para guardar experiencias.

Los resultados de la prueba de concepto presentada en este trabajo, 
muestran las ventajas de usar un modelo causal durante el entrenamiento de un agente. Entre ellas están: 
\begin{itemize}
    \item Alcanzar una recompensa mayor en un menor tiempo.
    \item Incluso un modelo incompleto (sin algunas relaciones del modelo verdadero) o incompleto y con relaciones falsas (al que denotaremos como incorrecto por simplicidad) mejoran el desempeño de un agente.
\end{itemize}

\section{Motivación y justificación}


El aprendizaje por refuerzo se ha convertido en los últimos años en una de las áreas
más investigadas dentro de la inteligencia artificial, psicología y neurociencia \cite{botvinick2019reinforcement}, incluso
se han generado agentes con un desempeño superior al humano en algunas tareas como  al jugar videojuegos \cite{mnih2015human, starcraft2019deepmind} y juegos de mesa como go y ajedrez \cite{Silver1140}.

El RL se acerca más que otros enfoques al tipo de aprendizaje que los humanos y otros
animales llevan a cabo y muchos de los principales algoritmos fueron inspirados por 
sistemas de aprendizaje biológicos \cite{sutton_barto_2018}. Sin embargo,
entre las principales diferencias de estos sistemas 
están, la cantidad de datos y el tiempo de entrenamiento
requeridos para aprender a realizar una tarea \cite{Silver1140}.

A pesar del éxito del RL, su teoría está basada 
en relaciones asociativas \cite{playingagainstnature2018}.
De acuerdo con \cite{pearl2018bookofwhy}, existen tres niveles de
habilidad cognitiva en un agente: \textit{observar}, \textit{hacer} e \textit{imaginar}.
% Y de acuerdo a estos niveles, 
% De acuerdo con los tres niveles
% de razonamiento causal definidos por \cite{pearl2018bookofwhy}, 
% las relaciones asociativas se encuentran en el primer nivel.
% Los tres niveles de razonamiento causal corresponden a

En el primer nivel se ubican las 
relaciones asociativas, donde se detectan regularidades
en el ambiente.
El segundo nivel, hacer, implica predecir los 
efectos de alteraciones intencionales sobre el ambiente
y elegir entre estas alteraciones  para producir una
salida deseada. En el tercer nivel, se encuentra
la retrospección e imaginación que
permite responder preguntas como: ¿por qué?
y ¿qué hubiera pasado si ...?

De acuerdo con Judea Pearl \cite{pearl2018bookofwhy}, 
un \textit{agente causal} debe dominar los tres niveles mencionados anteriormente.
El paradigma por interacción del RL, donde un agente
no sólo observa sino que también realiza acciones que 
cambian el estado del ambiente parece estar 
en el segundo nivel propuesto.
Un agente de RL puede explotar las relaciones causales presentes
en su ambiente con propósitos de manipulación y control \cite{woodward2005making}.
En específico, un agente de RL puede consultar un modelo
causal para preguntarse cosas como, ¿qué pasa si hago ...?
cada vez que selecciona una acción durante su entrenamiento.

% Examples are interventional questions: “What if I act?” and retrospective or explanatoryquestions: “What if I had acted differently?” No learning machine in operation today can answer suchquestions about interventions not encountered before, say, “What if we ban cigarettes.” Moreover, mostlearning machines today do not provide a representation from whichthe answers to such questions can bederived.

Actualmente, de acuerdo con \cite{pearl2018theoretical}, la mayoría de los agentes en funcionamiento no contestan preguntas sobre intervenciones que no han visto antes. 
Por lo tanto, es necesario equipar a
los algoritmos de aprendizaje con herramientas de razonamiento
causal para acelerar su aprendizaje y además alcanzar 
un desempeño al nivel de los humanos.
% Aunque quedan fuera de este trabajo de investigación el análisis y el aprovechamiento de las ventajas de utilizar un modelo causal para asistir el aprendizaje en vez de un modelo asociativo, es importante mencionar algunas de éstas:
Aunque existen otros tipos de representaciones para proveer de
conocimiento a un agente durante el proceso de aprendizaje, como serían
modelos asociativos, el utilizar modelos causales tiene ciertas
ventajas:

\begin{itemize}

\item Permite razonamiento contrafactual, esto es, brindar al agente la
capacidad de responder a preguntas como: ¿qué pasaría si tomara cierta
acción? 
%   Cosa, que no es posible con modelos asociativos.}
\item En general, 
%el conocimiento causal es genérico, así que sería fácil
el conocimiento causal es fácil de 
transferir a otras aplicaciones en dominios similares.
\item Puede ser más fácil que lo especifique una persona, ya que los seres humanos tienden a pensar en términos causales \cite{edmonds2018human, Gershman2017}.
\item Los modelos causales hacen a los sistemas más transparentes lo que permite que puedan explicar sus
decisiones.
\end{itemize}

Un ejemplo que muestra una ventaja de un modelo causal sobre uno asociativo es el siguiente.
Se tiene una tarea robótica que involucra las variables $T$, $L$ y $D$, que denotan tomar objeto, llevar objeto y
dejar objeto, respectivamente.
Un modelo causal podría representarse con las
siguientes relaciones: $T \rightarrow L \rightarrow D$.
Este bosquejo indica que se debe hacer la acción tomar antes de llevar y antes de
dejar. Por otra parte, un modelo asociativo no necesariamente tiene las direcciones
correctas. 
Desde una perspectiva asociativa, estos dos modelos son equivalente al anterior: $T \leftarrow L\leftarrow D$, $T\leftarrow L\rightarrow D$,
con respecto a las relaciones de independencia condicional.
Por lo tanto, éstos no necesariamente indican el orden correcto de las acciones o intervenciones que se deben
realizar.

Este trabajo explora un marco de trabajo donde un agente
de aprendizaje por refuerzo acelera su entrenamiento a través
del uso de un modelo causal, parcial o completo del ambiente
que habita. Fuera de los límites de
esta investigación queda
crear un agente inteligente que aprende y usa un modelo causal para tomar decisiones en retrospección o que
planifica y aprende imaginando ambientes hipotéticos.
Sin embargo, se buscan mostrar algunas de las ventajas de utilizar información extra obtenida de una estructura causal que 
describe al ambiente que habita un agente de RL.


\section{Problemática}

Un agente de RL debe aprender a mapear los estados
que percibe a acciones que se deben tomar cuando
se encuentre en esos estados. Este comportamiento
se infiere con la interacción a prueba y error con el ambiente y la única respuesta que el agente recibe después de llevar a cabo una acción, es una recompensa que mide que tan buena fue la acción ejecutada.
Este proceso puede ser muy lento en tareas donde no hay recompensas inmediatas, donde la mayoría de secuencias
de decisiones se basa en comportamientos no dirigidos
que producen una señal de recompensa nula.
Por lo tanto, para recibir una mayor recompensa, un agente debe intentar acciones que no haya \textit{explorado} o \textit{explotar} la experiencia que ya ha obtenido.

Normalmente, el explorar o explotar se realiza en el 
paso de la selección de acciones durante el entrenamiento de un agente. Restringir el espacio de búsqueda aprovechando 
propiedades del mundo y conocimiento que el agente tenga
de éste puede acelerar el entrenamiento.

Estas propiedades pueden ser relaciones causales 
que el agente tenga como conocimiento adicional
y que le permitan perseguir acciones que lo lleven a
estados deseados o evitar elegir acciones que lo lleven a 
estados no deseados. Un modelo causal parece una herramienta adecuada en
tareas de RL donde elegir una acción pueda representar una intervención en el modelo o la estructura causal del modelo permita conocer el orden causal de las acciones y estados.

Por otra parte, definir un modelo causal del ambiente no es trivial.
La información recibida por los agentes de RL en tareas actuales, a menudo está en un espacio de pixeles, es decir, perciben como imágenes a los estados del ambiente. Modelar las relaciones causales directamente de esos espacios de estados es intratable.
Por esta razón, el modelo causal puede representar sólo pequeñas partes del mundo del agente. Además, incluso cuando se trabajen sobre un espacio donde la información sea 
mucho más manejable, contar con los parámetros de un modelo causal puede ser una tarea muy difícil. Por lo tanto, se necesita una representación que no exceda por mucho la complejidad de los problemas de RL pero que proporcione información suficiente para ayudar a resolver la tarea.

% El problema de cómo representar el conocimiento causal, se representan acciones y estados

% El problema de cómo consultar el modelo

% El problema de como aprender el modelo

% Como se puede solucionar ese problema, una forma es guiar las acciones
% El seleccionar acciones buenas o malas te va a llevar a no equivocarte mucho

% La idea no es encontrar una política causal o una función de valor que ya existe
% sino auxiliar a que esta se aprenda más rapido.

% Aquí sería cosa de describir los problemas del lado del RL y del lado de la causalidad
% cómo se aprende un modelo es difícil. Desde donde empezamos, tenemos 
% la estructura pero no los parámetros o qué tenemos?

% El punto es llegar a decir que quiero guiar la selección de acciones.

\section{Preguntas de investigación}

% Las preguntas de investigación establecen lo que quiero 
% aprender.
\begin{enumerate}
%     \item ¿Se puede establecer una representación de las relaciones causales
% del ambiente que pueda ser utilizada por un agente en el proceso
% de aprendizaje por refuerzo?
\item ¿Se puede establecer una representación de las relaciones causales
del ambiente que pueda ser beneficiosa para un agente
de aprendizaje por refuerzo?
    % \item ¿Se puede añadir conocimiento causal al entrenamiento de un agente de RL?
    % \item ¿La reducción del espacio de búsqueda dado por una selección de acciones guiada por el modelo causal permite al agente tener un mejor desempeño durante el entrenamiento con respecto a una exploración a ciegas?
    
    \item ¿La acotación del espacio de búsqueda dada por una selección de acciones guiada por el modelo causal permite al agente reducir el tiempo de entrenamiento con respecto a una exploración a ciegas?
    % \item ¿Es posible aprender el modelo causal durante la interacción del
    % agente con el ambiente?
\end{enumerate}
\section{Hipótesis}

% La hipótesis en contraste, son sentencias de respuestas tentativas a esas preguntas
% Muchos establecen sus ideas sobre respuestas tentativas como
% parte del proceso de teorizar y analizar datos. Estas son proposiciones.
% Algunas relaciones causales en un ambiente se pueden capturar por medio
% de un grafo causal.
% Integrar esta estructura causal a un agente de RL para guiar sus acciones permite reducir su tiempo de entrenamiento.


Dada una tarea gobernada por un modelo causal y una representación
gráfica de dicho modelo, al integrar este conocimiento causal
a un agente que realiza aprendizaje por refuerzo, se reduce
el tiempo de aprendizaje respecto a una exploración a ciegas.



\section{Objetivo general}
El objetivo general de esta investigación
es establecer y validar un modelo computacional que integre 
un modelo causal a un algoritmo de RL; a través de
la selección de acciones guiada por un 
modelo causal en problemas que contienen
un mecanismo causal subyacente; para acelerar el entrenamiento de un agente sin decrementar la recompensa.

\section{Objetivos específicos}

Para alcanzar la meta de la investigación, se proponen
los siguientes objetivos:

\begin{itemize}
    \item Identificar ambientes en donde 
    las interacciones de un agente con éstos puedan ser representadas por un modelo causal.
    El modelo causal no necesariamente debe
    describir completamente cómo el ambiente puede responder a todas las acciones del agente.
    \item Definir una representación adecuada
    de un modelo causal para poder ser utilizada 
    por RL.
    % \item Investigar cómo un algoritmo de
    % RL puede consultar un modelo causal
    % para dirigir sus acciones.
    \item Implementar algoritmos de RL que consulten un modelo causal para dirigir sus acciones.
    \item Comparar el desempeño de algoritmos
    de RL usando la información del modelo y 
    sin usarla. 
\end{itemize}


\section{Contribuciones}


\begin{enumerate}
    \item Representación de la información del ambiente
    en un modelo causal para guiar a un agente de RL.
    \item 
    Formalización del esquema 
    de interacción entre
    un modelo causal y un algoritmo de aprendizaje por
    refuerzo.
    \item  Verificar empíricamente
    la mejora del desempeño de un agente usando diferentes niveles de información (completa, parcial e incorrecta) contenida en el modelo causal para motivar al siguiente paso 
    que es el descubrimiento causal durante la interacción con el
    ambiente.
\end{enumerate}

\section{Método propuesto y resultados}

El método propuesto en esta tesis se enfoca en guiar la
selección acciones de un agente de RL. De manera superficial y general, el método sigue un esquema similar al de la política $\epsilon$-greedy.
% , aunque puede adaptarse para otros enfoques de selección de acción.
El algoritmo propuesto recibe como entrada una estructura causal que describe las relaciones entre variables de estado y acciones.
Esta estructura causal puede ser definida a mano por un experto,
o incluso puede ser aprendida previamente, utilizando algoritmos como
el presentado en \cite{gonzalezsoto2020causal}.
Con cierta probabilidad se elige consultar el modelo, en otro caso, se elige la mejor acción según una función de valor o una política. 
Si el agente elige utilizar el modelo causal, entonces se obtiene una lista de aquellas variables que necesitan cambiar para aproximarse o alcanzar una meta de la tarea del agente. Para obtener una acción que acerca a la meta y además evitar aquellas que pueden desviar al agente, se consulta la estructura causal. La consulta consiste en obtener las acciones predecesoras de la lista de variables calculada en el paso anterior. Con esto, 
el agente puede realizar alguna acción deseable y evitar las que conducen a un comportamiento erróneo.

% En este trabajo, se incorpora el método propuesto en el algoritmo clásico de RL, Q-learning y se realizan diferentes experimentos.
Los resultados de los experimentos muestran, entre otras cosas, lo que es intuitivo, esto es, que una exploración guiada (incluso cuando no está completamente correcta) es mejor que una búsqueda a ciegas. Además, a pesar de que contar con un modelo completo parece resolver la tarea sin necesidad del aprendizaje, este es un escenario que no representa a la mayoría de los casos. Por lo tanto, se puede suponer que la información del modelo causal y el RL se complementan entre sí.

\section{Estructura del documento}

El documento del trabajo de investigación está estructurado de la 
siguiente forma.
En el capítulo \ref{chapter2} se exponen los fundamentos teóricos de ambas áreas, aprendizaje por refuerzo y causalidad.
En el capítulo \ref{chapter3} se describen de manera general algunos enfoques
que se han dado para acotar el espacio de búsqueda en problemas
de RL y las propuestas para unir el aprendizaje por refuerzo y
la causalidad. En el capítulo \ref{chapter4} se exponen y
delimitan las tareas a atacar y se describe el método propuesto para hacer frente a esos problemas. En el capítulo \ref{chapter5} se muestra una serie de experimentos
para diferentes configuraciones de los problemas. Finalmente,
el capítulo \ref{chapter6} incluye las conclusiones y algunas tareas pendientes que se buscan resolver.
