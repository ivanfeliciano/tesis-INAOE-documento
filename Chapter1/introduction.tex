\chapter{Introducción}\label{chapter1}



\graphicspath{{Chapter1/Figs/}}

Uno de los objetivos de la inteligencia artificial (IA), es crear agentes autónomos que aprendan
a través de la interacción con su ambiente \cite{surveyDRL2017}.
% La noción de aprender mediante interacciones es común para el ser humano.
% Éste extrae reglas y experiencia de la interacción con la naturaleza
% y utiliza esta información para cumplir una meta y mejorar su comportamiento con el paso del tiempo.
Un marco de trabajo para el aprendizaje por interacción es el \textit{aprendizaje por
refuerzo}.


El aprendizaje por refuerzo (RL por sus siglas en inglés) estudia cómo un agente puede aprender, a través de la percepción del estado de su ambiente,
a elegir acciones que maximicen recompensas futuras. 
Normalmente, no se le dice al agente qué acciones
tomar, en cambio, éste debe descubrir qué acciones, al intentarlas, producen la mayor recompensa.
En los casos más interesantes, las acciones no sólo afectan recompensas inmediatas
sino también el siguiente estado y, a través de eso, todas las recompensas ulteriores \cite{sutton_barto_2018}.

Los algoritmos de aprendizaje por refuerzo 
han mostrado ser efectivos en diversos dominios como en videojuegos \cite{mnih2013playing, vinyals2019grandmaster}, 
robótica \cite{openai2019solving}
y atención médica \cite{gottesman2018evaluating}.
Sin embargo, la mayoría de los  sistemas de aprendizaje por refuerzo actuales sufren de algunas deficiencias. Una de ellas es que no aprovechan procesos de
alto nivel para 
explotar patrones más allá de los asociativos \cite{garnelo2016deep, playingagainstnature2018}. Entre estos procesos de alto nivel se encuentra el \textit{razonamiento causal} \cite{pearl_2009, spirtes2000causation}. Las técnicas más comunes
en el aprendizaje por refuerzo ignoran \textit{relaciones causales} entre eventos.

Se tiene como hipótesis que conocer relaciones causales entre eventos podría permitir a un agente 
utilizar este conocimiento extra
para predecir qué causará realizar cierta acción
y entonces acelerar su proceso de aprendizaje.
Además, la capacidad de razonar sobre causas y efectos es parte integral de
la inteligencia natural \cite{nair2019causal}.


Como se menciona en \cite{theoryofcausalities2006} 
la causalidad no es un concepto monolítico y tampoco
existe una única entidad que sustente el uso correcto
del concepto. Existen muchas formas de definirla según diferentes contextos. Las teorías de causalidad
parecen ser un buen paradigma para algunas aplicaciones,
sin embargo, cada una tiene contraejemplos y problemas.
Entre algunos de los diferentes enfoques 
están; la causalidad para
el orden de eventos propuesta por Lamport \cite{lamport2019time} y otras teorías revisadas en
\cite{holland1986statistics} como la causalidad de Wiener-Granger para
series de tiempo \cite{granger1969investigating} y la 
causalidad probabilista de Suppes \cite{suppes1973probabilistic}. 

En este trabajo, la teoría
de causalidad que se sigue es la del enfoque \textit{intervencionista} propuesta
por Judea Pearl \cite{pearl2010introduction} \cite{sep-causal-models}.
En \cite{pearl2018bookofwhy} se describe un
nuevo paradigma de aprendizaje cuyo objetivo es descubrir relaciones de causa y efecto entre diferentes variables llamado \textit{inferencia causal} (CI por su siglas en inglés).
En esta área, la causalidad se define
como que $X$ causa $Y$ (denotado como $X \rightarrow Y$) si
y sólo si una intervención 
o manipulación de $X$ tiene un 
efecto en $Y$\cite{doi:10.1063/1.5025050}.
Una intervención va más allá que las observaciones pasivas pues asegura que el cambio
observado en $Y$ sea debido a $X$ y que no
es por una posible confusión con otros factores.

Las relaciones causales dentro de un sistema se pueden representar mediante un modelo matemático, conocido como modelo causal \cite{sep-causal-models}.
El modelo causal de un sistema
permite predecir qué pasaría si algunas
variables son intervenidas, predecir las salidas de casos que nunca han sido observados \cite{chaochao_2019} y responder preguntas como:
\textit{¿Si deseo esta salida, qué acción necesito
llevar a cabo?}


% Por qué es importante para una IA general


% Decir qué limitaciones tenemos

% y finalmente lo que proponemos, lo que buscamos



% % Se tiene como hipótesis que conocer tales relaciones podría permitir a un agente 
% % utilizar este conocimiento extra
% % para predecir qué causará realizar cierta acción
% % y entonces acelerar su proceso de aprendizaje.
% % Además, la capacidad de razonar sobre causas y efectos es parte integral de
% % la inteligencia natural \cite{nair2019causal}.



% \begin{itemize}
%     \item Poner el trabajo en una perspectiva mayor.
%     \item Visión de conjunto mostrando cómo el área del tema se relaciona con el resto del 
%     campo de las ciencias computacionales
%     \item Hacer un resumen de las ideas clave del trabajo, en particular resaltando 
%     los beneficios de mi trabajo.
%     \item La introducción contiene los siguientes elementos:
%     \begin{itemize}
%         \item \textbf{Tema:} indicando el \textit{tema en específico} del reporte; esto
%         se debe hacer tan pronto como sea posible, preferentemente en el primer
%         párrafo del reporte.
%         \item \textbf{Propósito y situación:} indicando por qué el reporte fue escrito
%         y cuál fue el propósito.
%         \item \textbf{Antecedentes del tema:} provee definiciones clave
%         y preliminares básicos importantes para el lector; debe 
%         capturar el interés del lector.
%         \item \textbf{Visión general del reporte:} describe el 
%         esquema general del reporte. Esto informa al lector
%         que esperar, y hace más fácil entender el material y hacer 
%         transiciones entre secciones.
        
%     \end{itemize}
    
% \end{itemize}






Este razonamiento causal puede hacer 
que los agentes de RL, o sistemas de inteligencia artificial en general, 
sean más eficientes.
Por ejemplo, un robot que entiende que dejar caer cosas hace que se rompan, no necesitaría tirar cientos de objetos frágiles al suelo para ver qué les sucede.




% ¿Cuáles son los objetivos y las principales metas de investigación y porqué son importantes?

En este trabajo de investigación el objetivo es 
acelerar el entrenamiento de un agente de RL
a través de la selección de acciones guiadas por un 
modelo causal para completar tareas que tienen una
estructura causal subyacente.

En general, un agente comienza su búsqueda a ciegas,
mediante interacciones a prueba y error.
Sin embargo, éste puede, a través de intervenciones
en un modelo causal, hacer consultas del tipo: \textit{¿Qué tal si hago ...?} Estas intervenciones sobre variables 
que describen a las acciones permiten usar al modelo
como ``oráculo'' para no realizar acciones que lleven
a estados no deseados o para no evitar la acción que
lleve a una meta.

Se ha probado que la formulación actual del aprendizaje
por refuerzo no es un problema causal \cite{gonzalezsoto2019reinforcement}.
Por lo tanto, es una suposición fuerte 
decir que todos los problemas 
en aprendizaje por refuerzo contienen mecanismos causales. 
Sin embargo, hay tareas en las
que un experto o incluso el mismo algoritmo puede
aprender el modelo causal latente.
Problemas con una estructura causal latente
se pueden encontrar en robótica, ambientes como Animal AI \cite{beyret2019animalai}, 
tareas dirigidas por metas \cite{nair2019causal} e
incluso algunos juegos \cite{madumal2019explainable}.


% ¿Qué aspectos específicos del problema general se atacarán?
% % ¿Cuál es la razón o justificación para seguir esta línea de investigación?
% ¿Se tienen conclusiones o resultados 
% preliminares?, ¿cuáles y de qué tipo son?

Los autores en  \cite{lattimore2016causal} y \cite{nair2019causal} han mostrado
que es posible mejorar la identificación de acciones que proporcionan una recompensa mayor en
el problema del bandido y 
que una estructura causal permite
que una política condicionada a una
meta (aprendida mediante aprendizaje por imitación) generalice para instancias no vistas de 
un problema, respectivamente.
Sin embargo, a diferencia de estos trabajos,
en esta investigación 
se propone que el modelo causal se suponga 
aprendido y se ataquen problemas con ambientes discretos 
y continuos. El modelo permite una selección de acciones 
guiada para la inmediata actualización de la política
o para guardar experiencias.

Los resultados de la prueba de concepto presentada en este trabajo, 
muestran las ventajas de usar un modelo causal durante el entrenamiento de un agente. Entre ellas están: 
\begin{itemize}
    \item Alcanzar una recompensa mayor en un menor tiempo.
    \item Incluso un modelo incompleto o incorrecto mejoran el desempeño de un agente.
\end{itemize}

\section{Motivación y justificación}


El aprendizaje por refuerzo se ha convertido en los últimos años en una de las áreas
más investigadas dentro de la inteligencia artificial, psicología y neurociencia \cite{botvinick2019reinforcement}, incluso
se han generado agentes con un desempeño superior al humano en algunas tareas como  al jugar videojuegos \cite{mnih2015human, starcraft2019deepmind} y juegos de mesa como go y ajedrez \cite{Silver1140}.

El RL se acerca más que otros enfoques al tipo de aprendizaje que los humanos y otros
animales llevan a cabo y muchos de los principales algoritmos fueron inspirados por 
sistemas de aprendizaje biológicos \cite{sutton_barto_2018}. Sin embargo,
entre las principales diferencias de estos sistemas 
están, la cantidad de datos y el tiempo de entrenamiento
requeridos para aprender a realizar una tarea \cite{Silver1140}.

A pesar del éxito del RL, su teoría está basada 
en relaciones asociativas \cite{playingagainstnature2018}.
De acuerdo con los tres niveles
de razonamiento causal definidos por \cite{pearl2018bookofwhy}, las 
relaciones asociativas se encuentran en el primer nivel.
Los tres niveles de razonamiento causal corresponden a: \textit{observar}, \textit{intervenir}
e \textit{imaginar}.
En el primer nivel se ubican las 
relaciones asociativas, donde se detectan regularidades
en el ambiente a partir de observaciones pasivas de los datos.
El segundo nivel, intervenir, implica predecir los 
efectos de alteraciones intencionales sobre el ambiente
y elegir entre estas alteraciones  para producir una
salida deseada. En el tercer nivel se encuentra
el razonamiento \textit{contrafactual} donde la retrospección
e imaginación permite responde preguntas como: ¿por qué?
y ¿qué hubiera pasado si una intervención se hubiera hecho 
en vez de otra?

El paradigma por interacción del RL, donde un agente
no sólo observa sino que también realiza acciones que 
cambian el estado del ambiente parece estar 
en el segundo nivel propuesto por \cite{pearl2018bookofwhy}.
Un agente puede explotar las relaciones causales presentes
en su ambiente con propósitos de manipulación y control \cite{woodward2005making}.
En específico, un agente de RL puede consultar un modelo
causal para preguntarse cosas como, ¿qué pasa si hago ...?
cada vez que selecciona una acción durante su entrenamiento.

Hoy en día, ningún agente inteligente puede contestar 
preguntas sobre intervenciones que no ha visto antes 
\cite{pearl2018theoretical}. Por lo tanto, es necesario equipar a
los algoritmos de aprendizaje con herramientas de razonamiento
causal para acelerar su aprendizaje y además alcanzar 
un desempeño al nivel de los humanos.
Este trabajo explora un marco de trabajo donde un agente
de aprendizaje por refuerzo acelera su entrenamiento a través
del uso de un modelo causal, parcial o completo del ambiente
que habita. Fuera de los límites de
esta investigación queda
crear un agente inteligente que aprende y usa un modelo causal para tomar decisiones en retrospección o que
planifica y aprende imaginando ambientes hipotéticos.
Sin embargo, se buscan mostrar algunas de las ventajas de utilizar información extra obtenida de una estructura causal que 
describe al ambiente que habita un agente de RL.


\section{Problemática}

Un agente de RL debe aprender a mapear los estados
que percibe a acciones que se deben tomar cuando
se encuentre en esos estados. Este comportamiento
se infiere con la interacción a prueba y error con el ambiente y la única señal que el agente recibe es una recompensa que mide que tan buena fue la acción que llevó a cabo. Es por eso que este proceso puede ser muy lento en tareas donde no hay recompensas inmediatas, donde la mayoría de secuencias
de decisiones no inteligentes producen una señal de recompensa nula.
Por lo tanto, para recibir una mayor recompensa, un agente debe intentar acciones que no haya \textit{explorado} o \textit{explotar} la experiencia que ya ha obtenido.

Normalmente, el explorar o explotar se realiza en el 
paso de la selección de acciones durante el entrenamiento de un agente. Restringir el espacio de búsqueda aprovechando 
propiedades del mundo y conocimiento que el agente tenga
de éste puede acelerar el entrenamiento.

Estas propiedades pueden ser relaciones causales 
que el agente tenga como conocimiento adicional
y que le permitan perseguir acciones que lo lleven a
estados deseados o evitar elegir acciones que lo lleven a 
estados no deseados. Un modelo causal parece una herramienta adecuada en
tareas de RL donde elegir una acción pueda representar una intervención en el modelo o la estructura causal del modelo permita conocer el orden causal de las acciones y estados.

Por otra parte, definir un modelo causal del ambiente no es trivial.
Las tareas actuales en las que se aplican soluciones 
basadas en RL tienen como características que la información
que recibe el agente normalmente es de alta dimensionalidad, por 
ejemplo, imágenes. Un modelo causal donde se represente toda
la información del ambiente con el que interactúa el agente
es intratable. 
Por esta razón, el modelo causal puede representar sólo pequeñas partes del mundo del agente. Además, incluso cuando se trabajen sobre un espacio donde la información sea 
mucho más manejable, contar con los parámetros de un modelo causal puede ser una tarea muy difícil. Por lo tanto, se necesita una representación que no exceda por mucho la complejidad de los problemas de RL pero que proporcione información suficiente para ayudar a resolver la tarea.

% El problema de cómo representar el conocimiento causal, se representan acciones y estados

% El problema de cómo consultar el modelo

% El problema de como aprender el modelo

% Como se puede solucionar ese problema, una forma es guiar las acciones
% El seleccionar acciones buenas o malas te va a llevar a no equivocarte mucho

% La idea no es encontrar una política causal o una función de valor que ya existe
% sino auxiliar a que esta se aprenda más rapido.

% Aquí sería cosa de describir los problemas del lado del RL y del lado de la causalidad
% cómo se aprende un modelo es difícil. Desde donde empezamos, tenemos 
% la estructura pero no los parámetros o qué tenemos?

% El punto es llegar a decir que quiero guiar la selección de acciones.

\section{Preguntas de investigación}

% Las preguntas de investigación establecen lo que quiero 
% aprender.
\begin{enumerate}
    \item ¿Se puede establecer una representación de las relaciones causales
del ambiente que pueda ser utilizada por un agente en el proceso
de aprendizaje por refuerzo?
    % \item ¿Se puede añadir conocimiento causal al entrenamiento de un agente de RL?
    \item ¿La reducción del espacio de búsqueda dado por una selección de acciones guiada por el modelo causal permite al agente tener un mejor desempeño durante el entrenamiento con respecto a una exploración a ciegas?
    % \item ¿Es posible aprender el modelo causal durante la interacción del
    % agente con el ambiente?
\end{enumerate}
\section{Hipótesis}

% La hipótesis en contraste, son sentencias de respuestas tentativas a esas preguntas
% Muchos establecen sus ideas sobre respuestas tentativas como
% parte del proceso de teorizar y analizar datos. Estas son proposiciones.
% Algunas relaciones causales en un ambiente se pueden capturar por medio
% de un grafo causal.
% Integrar esta estructura causal a un agente de RL para guiar sus acciones permite reducir su tiempo de entrenamiento.


Dada una tarea gobernada por un modelo causal y una representación
gráfica de dicho modelo, al integrar este conocimiento causal
a un agente que realiza aprendizaje por refuerzo, se reduce
el tiempo de aprendizaje respecto a una exploración a ciegas.

\section{Objetivo general}
El objetivo general de esta investigación
es establecer y validar un modelo computacional que integre 
un modelo causal a un algoritmo de RL; a través de
la selección de acciones guiada por un 
modelo causal en problemas que contienen
un mecanismo causal subyacente; para acelerar el entrenamiento de un agente.
\section{Objetivos específicos}

Para alcanzar la meta de la investigación, se proponen
los siguientes objetivos:

\begin{itemize}
    \item Identificar ambientes en donde 
    las interacciones de un agente con éstos puedan ser representadas por un modelo causal.
    El modelo causal no necesariamente debe
    describir completamente cómo el ambiente puede responder a todas las acciones del agente.
    \item Definir una representación adecuada
    de un modelo causal para poder ser utilizada 
    por RL.
    % \item Investigar cómo un algoritmo de
    % RL puede consultar un modelo causal
    % para dirigir sus acciones.
    \item Implementar algoritmos de RL que consulten un modelo causal para dirigir sus acciones.
    \item Comparar el desempeño de algoritmos
    de RL usando la información del modelo y 
    sin usarla. 
\end{itemize}


\section{Contribuciones}


\begin{enumerate}
    \item Representación de la información del ambiente
    en un modelo causal para guiar a un agente de RL.
    \item 
    Formalización del esquema 
    de interacción entre
    un modelo causal y un algoritmo de aprendizaje por
    refuerzo.
    \item Experimentación que verifica empíricamente
    la mejora del desempeño de un agente usando diferentes niveles de información (completa, parcial e incorrecta) contenida en el modelo causal para motivar al siguiente paso 
    que es el descubrimiento causal durante la interacción con el
    ambiente.
\end{enumerate}


\section{Estructura del documento}

El documento del trabajo de investigación está estructurado de la 
siguiente forma.
En el capítulo \ref{chapter2} se exponen los fundamentos teóricos de ambas áreas, aprendizaje por refuerzo y causalidad.
En el capítulo \ref{chapter3} se describen de manera general algunos enfoques
que se han dado para acotar el espacio de búsqueda en problemas
de RL y las propuestas para unir el aprendizaje por refuerzo y
la causalidad. En el capítulo \ref{chapter4} se exponen y
delimitan las tareas a atacar y se describe el método propuesto para hacer frente a esos problemas. En el capítulo \ref{chapter5} se muestra una serie de experimentos
para diferentes configuraciones de los problemas. Finalmente,
el capítulo \ref{chapter6} incluye las conclusiones y algunas tareas pendientes que se buscan resolver.
